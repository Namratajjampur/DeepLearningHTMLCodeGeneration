{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this program does\n",
    "Given 2 numbers a and b of any length, the program learns to compute the sum.\n",
    "So dataset has to be created of suitable size(bits). Then they need to be fed to the GRU to learn the sum.\n",
    "<br>\n",
    "\n",
    "It has to learn that <br>\n",
    "3+3 = 6 <br>\n",
    "so 3=11 -> [1,1,0,0,0] <br>\n",
    "and 6=110 -> [0,1,1,0,0] <br>\n",
    "\n",
    "So it learns that given input: [1,1,0,0,0] and [1,1,0,0,0] <br>\n",
    "it should get [0,1,1,0,0] as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5, 2)\n",
      "(100, 5, 2)\n",
      "(16, 3, 2)\n",
      "[0. 1. 0.] [0. 1. 0.] [0. 0. 1.]\n",
      "[0. 1. 0.] [0. 0. 0.] [0. 1. 0.]\n",
      "[0. 1. 0.] [1. 0. 0.] [1. 1. 0.]\n",
      "[0. 0. 0.] [1. 0. 0.] [1. 0. 0.]\n",
      "[1. 0. 0.] [1. 0. 0.] [0. 1. 0.]\n",
      "[1. 0. 0.] [0. 1. 0.] [1. 1. 0.]\n",
      "[0. 1. 0.] [0. 1. 0.] [0. 0. 1.]\n",
      "[0. 0. 0.] [0. 1. 0.] [0. 1. 0.]\n",
      "[1. 0. 0.] [0. 0. 0.] [1. 0. 0.]\n",
      "[0. 1. 0.] [0. 1. 0.] [0. 0. 1.]\n",
      "[0. 1. 0.] [0. 0. 0.] [0. 1. 0.]\n",
      "[0. 0. 0.] [1. 0. 0.] [1. 0. 0.]\n",
      "[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]\n",
      "[0. 0. 0.] [0. 1. 0.] [0. 1. 0.]\n",
      "[1. 0. 0.] [1. 0. 0.] [0. 1. 0.]\n",
      "[1. 0. 0.] [0. 1. 0.] [1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def as_bytes(num, final_size):\n",
    "    \"\"\"Converts an integer to a reversed bitstring (of size final_size).\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    num: int\n",
    "        The number to convert.\n",
    "    final_size: int\n",
    "        The length of the bitstring.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list:\n",
    "        A list which is the reversed bitstring representation of the given number.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> as_bytes(3, 4)\n",
    "    [1, 1, 0, 0]\n",
    "    >>> as_bytes(3, 5)\n",
    "    [1, 1, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for _ in range(final_size):\n",
    "        res.append(num % 2)\n",
    "        num //= 2\n",
    "    return res\n",
    "\n",
    "# @@ WHY DO YOU NEED REVERSE BITSTRING???\n",
    "\n",
    "\n",
    "def generate_example(num_bits):\n",
    "    \"\"\"Generate an example addition.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a: list\n",
    "        The first term (represented as reversed bitstring) of the addition.\n",
    "    b: list\n",
    "        The second term (represented as reversed bitstring) of the addition.\n",
    "    c: list\n",
    "        The addition (a + b) represented as reversed bitstring.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> np.random.seed(4)\n",
    "    >>> a, b, c = generate_example(3)\n",
    "    >>> a\n",
    "    [0, 1, 0]\n",
    "    >>> b\n",
    "    [0, 1, 0]\n",
    "    >>> c\n",
    "    [0, 0, 1]\n",
    "    >>> # Notice that these numbers are represented as reversed bitstrings)\n",
    "    \"\"\"\n",
    "    a = random.randint(0, 2**(num_bits - 1) - 1)\n",
    "    b = random.randint(0, 2**(num_bits - 1) - 1)\n",
    "    res = a + b\n",
    "    return (as_bytes(a,  num_bits),\n",
    "            as_bytes(b,  num_bits),\n",
    "            as_bytes(res,num_bits))\n",
    "\n",
    "# @@This function will be used to generate a dataset.\n",
    "\n",
    "def generate_batch(num_bits, batch_size):\n",
    "    \"\"\"Generates instances of the addition problem.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use for each number.\n",
    "    batch_size: int\n",
    "        The number of examples to generate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x: np.array\n",
    "        Two numbers to be added represented as bits (in reversed order).\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is one of [0,1] depending for first and second summand respectively.\n",
    "    y: np.array\n",
    "        The result of the addition.\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is always 0 since there is only one result.\n",
    "    \"\"\"\n",
    "    x = np.empty((batch_size, num_bits, 2))\n",
    "    y = np.empty((batch_size, num_bits, 1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a, b, r = generate_example(num_bits)\n",
    "        x[i, :, 0] = a\n",
    "        x[i, :, 1] = b\n",
    "        y[i, :, 0] = r\n",
    "    print(np.shape(x))\n",
    "    return x, y\n",
    "\n",
    "# @@ This function just calls the generate function many times to make a dataset. \n",
    "# @@ It then stores it in the required form\n",
    "\n",
    "# Configuration\n",
    "batch_size = 100\n",
    "time_size = 5\n",
    "\n",
    "X_train, Y_train = generate_batch(time_size, batch_size)\n",
    "X_test, Y_test = generate_batch(time_size, batch_size)\n",
    "\n",
    "# @@ time size is actually the bit length\n",
    "# @@ WHY IS IT NAMED TIME_SIZE???\n",
    "\n",
    "# px,py = generate_batch(3,16)\n",
    "# for i in range(len(px)):\n",
    "#     print(px[i,:,0],px[i,:,1],py[i,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, train loss: 1.2067, test loss: 1.2235\n",
      "Iteration: 50, train loss: 0.8700, test loss: 0.8717\n",
      "Iteration: 100, train loss: 0.6776, test loss: 0.6810\n",
      "Iteration: 150, train loss: 0.6606, test loss: 0.6616\n",
      "Iteration: 200, train loss: 0.6506, test loss: 0.6504\n",
      "Iteration: 250, train loss: 0.6427, test loss: 0.6420\n",
      "Iteration: 300, train loss: 0.6360, test loss: 0.6351\n",
      "Iteration: 350, train loss: 0.6300, test loss: 0.6290\n",
      "Iteration: 400, train loss: 0.6245, test loss: 0.6235\n",
      "Iteration: 450, train loss: 0.6193, test loss: 0.6182\n",
      "Iteration: 500, train loss: 0.6143, test loss: 0.6129\n",
      "Iteration: 550, train loss: 0.6091, test loss: 0.6074\n",
      "Iteration: 600, train loss: 0.6038, test loss: 0.6015\n",
      "Iteration: 650, train loss: 0.5980, test loss: 0.5951\n",
      "Iteration: 700, train loss: 0.5918, test loss: 0.5880\n",
      "Iteration: 750, train loss: 0.5849, test loss: 0.5801\n",
      "Iteration: 800, train loss: 0.5772, test loss: 0.5713\n",
      "Iteration: 850, train loss: 0.5687, test loss: 0.5615\n",
      "Iteration: 900, train loss: 0.5592, test loss: 0.5507\n",
      "Iteration: 950, train loss: 0.5488, test loss: 0.5387\n",
      "Iteration: 1000, train loss: 0.5371, test loss: 0.5255\n",
      "Iteration: 1050, train loss: 0.5233, test loss: 0.5104\n",
      "Iteration: 1100, train loss: 0.5056, test loss: 0.4924\n",
      "Iteration: 1150, train loss: 0.4833, test loss: 0.4712\n",
      "Iteration: 1200, train loss: 0.4536, test loss: 0.4393\n",
      "Iteration: 1250, train loss: 0.3996, test loss: 0.3774\n",
      "Iteration: 1300, train loss: 0.3058, test loss: 0.2725\n",
      "Iteration: 1350, train loss: 0.2450, test loss: 0.2042\n",
      "Iteration: 1400, train loss: 0.2143, test loss: 0.1762\n",
      "Iteration: 1450, train loss: 0.1921, test loss: 0.1592\n",
      "Iteration: 1500, train loss: 0.1754, test loss: 0.1460\n",
      "Iteration: 1550, train loss: 0.1612, test loss: 0.1332\n",
      "Iteration: 1600, train loss: 0.1481, test loss: 0.1211\n",
      "Iteration: 1650, train loss: 0.1359, test loss: 0.1103\n",
      "Iteration: 1700, train loss: 0.1244, test loss: 0.1003\n",
      "Iteration: 1750, train loss: 0.1141, test loss: 0.0919\n",
      "Iteration: 1800, train loss: 0.1060, test loss: 0.0858\n",
      "Iteration: 1850, train loss: 0.1000, test loss: 0.0816\n",
      "Iteration: 1900, train loss: 0.0955, test loss: 0.0785\n",
      "Iteration: 1950, train loss: 0.0918, test loss: 0.0760\n",
      "Iteration: 2000, train loss: 0.0886, test loss: 0.0738\n",
      "Iteration: 2050, train loss: 0.0852, test loss: 0.0715\n",
      "Iteration: 2100, train loss: 0.0809, test loss: 0.0687\n",
      "Iteration: 2150, train loss: 0.0751, test loss: 0.0647\n",
      "Iteration: 2200, train loss: 0.0656, test loss: 0.0571\n",
      "Iteration: 2250, train loss: 0.0498, test loss: 0.0435\n",
      "Iteration: 2300, train loss: 0.0299, test loss: 0.0262\n",
      "Iteration: 2350, train loss: 0.0144, test loss: 0.0126\n",
      "Iteration: 2400, train loss: 0.0067, test loss: 0.0060\n",
      "Iteration: 2450, train loss: 0.0035, test loss: 0.0035\n",
      "Iteration: 2500, train loss: 0.0022, test loss: 0.0024\n",
      "Iteration: 2550, train loss: 0.0016, test loss: 0.0018\n",
      "Iteration: 2600, train loss: 0.0012, test loss: 0.0014\n",
      "Iteration: 2650, train loss: 0.0009, test loss: 0.0012\n",
      "Iteration: 2700, train loss: 0.0008, test loss: 0.0010\n",
      "Iteration: 2750, train loss: 0.0007, test loss: 0.0008\n",
      "Iteration: 2800, train loss: 0.0006, test loss: 0.0007\n",
      "Iteration: 2850, train loss: 0.0005, test loss: 0.0007\n",
      "Iteration: 2900, train loss: 0.0004, test loss: 0.0006\n",
      "Iteration: 2950, train loss: 0.0004, test loss: 0.0005\n",
      "Iteration: 3000, train loss: 0.0004, test loss: 0.0005\n",
      "Iteration: 3050, train loss: 0.0003, test loss: 0.0005\n",
      "Iteration: 3100, train loss: 0.0003, test loss: 0.0004\n",
      "Iteration: 3150, train loss: 0.0003, test loss: 0.0004\n",
      "Iteration: 3200, train loss: 0.0003, test loss: 0.0004\n",
      "Iteration: 3250, train loss: 0.0002, test loss: 0.0003\n",
      "Iteration: 3300, train loss: 0.0002, test loss: 0.0003\n",
      "Iteration: 3350, train loss: 0.0002, test loss: 0.0003\n",
      "Iteration: 3400, train loss: 0.0002, test loss: 0.0003\n",
      "Iteration: 3450, train loss: 0.0002, test loss: 0.0003\n",
      "Iteration: 3500, train loss: 0.0002, test loss: 0.0003\n",
      "Iteration: 3550, train loss: 0.0002, test loss: 0.0002\n",
      "Iteration: 3600, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3650, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3700, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3750, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3800, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3850, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3900, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 3950, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 4000, train loss: 0.0001, test loss: 0.0002\n",
      "Iteration: 4050, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4100, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4150, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4200, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4250, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4300, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4350, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4400, train loss: 0.0001, test loss: 0.0001\n",
      "Iteration: 4450, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4500, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4550, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4600, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4650, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4700, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4750, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4800, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4850, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4900, train loss: 0.0000, test loss: 0.0001\n",
      "Iteration: 4950, train loss: 0.0000, test loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "    \n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "        \n",
    "        return h_t\n",
    "    \n",
    "#%% (3) Initialize and train the model.\n",
    "\n",
    "# The input has 2 dimensions: dimension 0 is reserved for the first term and dimension 1 is reverved for the second term\n",
    "input_dimensions = 2\n",
    "\n",
    "# @@ SO IN OUR CASE INPUT DIMENSION IS NUMBER OF FEATURES CNN HAS IDENTIFIED\n",
    "# @@ WHAT IS IT IN OUT CASE CORRESPONDING TO THEIR % OR TIME_SIZE?? \n",
    "\n",
    "# Arbitrary number for the size of the hidden state\n",
    "hidden_size = 16\n",
    "\n",
    "# Initialize a session\n",
    "session = tf.Session()\n",
    "\n",
    "# Create a new instance of the GRU model\n",
    "gru = GRU(input_dimensions, hidden_size)\n",
    "\n",
    "# Add an additional layer on top of each of the hidden state outputs\n",
    "W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "# Create a placeholder for the expected output\n",
    "expected_output = tf.placeholder(dtype=tf.float64, shape=(batch_size, time_size, 1), name='expected_output')\n",
    "\n",
    "# Just use quadratic loss\n",
    "loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output, 2)) / float(batch_size)\n",
    "# THEY'VE DONE RMS ERROR\n",
    "\n",
    "# Use the Adam optimizer for training\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all the variables\n",
    "# @@ WHAT IS THIS AND WHY DO WE NEED IT?\n",
    "init_variables = tf.global_variables_initializer()\n",
    "session.run(init_variables)\n",
    "\n",
    "# Initialize the losses\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "# @@ THIS IS TO KEEP TRACK OF LOSSES ACROSS ITERATIONS\n",
    "\n",
    "\n",
    "# Perform all the iterations\n",
    "for epoch in range(5000):\n",
    "    # Compute the losses\n",
    "    _, train_loss = session.run([train_step, loss], feed_dict={gru.input_layer: X_train, expected_output: Y_train})\n",
    "    validation_loss = session.run(loss, feed_dict={gru.input_layer: X_test, expected_output: Y_test})\n",
    "    \n",
    "    # Log the losses\n",
    "    train_losses += [train_loss]\n",
    "    validation_losses += [validation_loss]\n",
    "    \n",
    "    # Display an update every 50 iterations\n",
    "    if epoch % 100 == 0:\n",
    "#         plt.plot(train_losses, '-b', label='Train loss')\n",
    "#         plt.plot(validation_losses, '-r', label='Validation loss')\n",
    "#         plt.legend(loc=0)\n",
    "#         plt.title('Loss')\n",
    "#         plt.xlabel('Iteration')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.show()\n",
    "        print('Iteration: %d, train loss: %.4f, test loss: %.4f' % (epoch, train_loss, validation_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually checking how good model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_custom_sample:\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "\n",
      "\n",
      "\n",
      "X_custom:\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n",
      "\n",
      "\n",
      "\n",
      "X_custom new:\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 1.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [1. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n",
      "\n",
      "\n",
      "\n",
      "ypred: [[[-1.23365697e-04]\n",
      "  [ 4.48938901e-04]\n",
      "  [-2.19935621e-03]\n",
      "  [-2.03709776e-02]\n",
      "  [ 1.00847784e+00]\n",
      "  [-1.20020444e-03]\n",
      "  [-5.77053686e-03]\n",
      "  [ 3.70563604e-02]\n",
      "  [-2.91676604e-02]\n",
      "  [-9.31812185e-04]\n",
      "  [ 1.01434215e+00]\n",
      "  [-1.34618859e-02]\n",
      "  [ 1.15447770e-02]\n",
      "  [ 3.79115673e-02]\n",
      "  [-2.94113193e-02]\n",
      "  [-4.71613237e-04]\n",
      "  [-2.26599578e-02]\n",
      "  [-1.55177820e-02]\n",
      "  [-1.89746890e-02]\n",
      "  [-1.88357394e-02]]]\n",
      "y_bits: [0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "y_bitstr: 00000000010000010000\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "# Define two numbers a and b and let the model compute a + b\n",
    "a = 1024\n",
    "b = 16\n",
    "\n",
    "# The model is independent of the sequence length! Now we can test the model on even longer bitstrings\n",
    "bitstring_length = 20\n",
    "\n",
    "# @@ 1024 = 10,00,00,00,00,0 so here it would become 0,00,00,00,00,01|00,00,00,00,0\n",
    "# @@ 16 = 1,00,00 so here it will be 00,00,1|00,00,00,00,00,00,00,0\n",
    "\n",
    "# Create the feature vectors    \n",
    "X_custom_sample = np.vstack([as_bytes(a, bitstring_length), as_bytes(b, bitstring_length)]).T\n",
    "print(\"X_custom_sample:\")\n",
    "print(X_custom_sample)\n",
    "# @@ this give in the form: [[a],[b]]\n",
    "print(\"\\n\\n\")\n",
    "X_custom = np.zeros((1,) + X_custom_sample.shape)\n",
    "# @@ this is done because [[[a],[b]]] is input to GRU\n",
    "print(\"X_custom:\")\n",
    "print(X_custom)\n",
    "print(\"\\n\\n\")\n",
    "X_custom[0, :, :] = X_custom_sample\n",
    "print(\"X_custom new:\")\n",
    "print(X_custom)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Make a prediction by using the model\n",
    "y_predicted = session.run(output, feed_dict={gru.input_layer: X_custom})\n",
    "print(\"ypred:\",y_predicted)\n",
    "# Just use a linear class separator at 0.5\n",
    "y_bits = 1 * (y_predicted > 0.5)[0, :, 0]\n",
    "print(\"y_bits:\",y_bits)\n",
    "# Join and reverse the bitstring\n",
    "y_bitstr = ''.join([str(int(bit)) for bit in y_bits.tolist()])[::-1]\n",
    "print(\"y_bitstr:\",y_bitstr)\n",
    "# Convert the found bitstring to a number\n",
    "y = int(y_bitstr, 2)\n",
    "\n",
    "# Print out the prediction\n",
    "print(y) # Yay! This should equal 1024 + 16 = 1040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
