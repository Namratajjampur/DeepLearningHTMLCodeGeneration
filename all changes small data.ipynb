{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "    \n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dimensions, hidden_size, inputs,dtype=tf.float32):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_layer=[]\n",
    "        \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = inputs\n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float32, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "#         print(self.x_t.shape)\n",
    "        \n",
    "        print(\"h_t:\",h_t.shape)\n",
    "#         print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vb_size=18\n",
    "batchsize=2\n",
    "weights = {\n",
    "    'W_conv1': tf.get_variable('W0', shape=(3,3,3,32), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv2': tf.get_variable('W1', shape=(3,3,32,32), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv3': tf.get_variable('W2', shape=(3,3,32,64), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv4': tf.get_variable('W3', shape=(3,3,64,64), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv5': tf.get_variable('W4', shape=(3,3,64,128), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv6': tf.get_variable('W5', shape=(3,3,128,128), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_fc1': tf.get_variable('W6', shape=(28*28*128,1024), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_fc2': tf.get_variable('W7', shape=(1024,1024), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'Wout_gru1': tf.get_variable('W8', dtype = tf.float32,shape=(256,256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Wout_gru_1': tf.get_variable('W10', dtype = tf.float32,shape=(256,256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Wout_gru2': tf.get_variable('W9', dtype = tf.float32,shape=(512,vb_size), initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    }\n",
    "biases = {\n",
    "    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc2': tf.get_variable('B1', shape=(32), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc3': tf.get_variable('B2', shape=(64), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc4': tf.get_variable('B3', shape=(64), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc5': tf.get_variable('B4', shape=(128), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc6': tf.get_variable('B5', shape=(128), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'b_fc1': tf.get_variable('B6', shape=(1024), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'b_fc2': tf.get_variable('B7', shape=(1024), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Bout_gru1':tf.get_variable('B8', dtype = tf.float32,shape=(256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Bout_gru_1':tf.get_variable('B10', dtype = tf.float32,shape=(256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Bout_gru2': tf.get_variable('B9', dtype = tf.float32,shape=(vb_size), initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn_test(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    \n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"********\",weights['W_conv1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "#    conv2 = tf.nn.dropout(conv2, 0.25)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    "#    conv4 = tf.nn.dropout(conv4, 0.25)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "#    conv6 = tf.nn.dropout(conv6, 0.25)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "#    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "#    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "#     x_norm = batch_norm_wrapper(fc2,is_training)\n",
    "    \n",
    "    \n",
    "#     inputs=fc2\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     decay=0.9999\n",
    "\n",
    "#     if is_training:\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "#     else:\n",
    "#         return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "#     x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "#     print(x_norm.shape)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn_train(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    \n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"********\",weights['W_conv1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "#    conv2 = tf.nn.dropout(conv2, 0.2)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    " #   conv4 = tf.nn.dropout(conv4, 0.2)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    " #   conv6 = tf.nn.dropout(conv6, 0.2)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "#     x_norm = batch_norm_wrapper(fc2,is_training)\n",
    "    \n",
    "    \n",
    "#     inputs=fc2\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     decay=0.9999\n",
    "\n",
    "#     if is_training:\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "#     else:\n",
    "#         return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "#     x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "#     print(x_norm.shape)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "import cv2\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def resize_img(png_file_path):\n",
    "        img_rgb = cv2.imread(png_file_path)\n",
    "        #img_grey = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "        #img_adapted = cv2.adaptiveThreshold(img_grey, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY, 101, 9)\n",
    "        #img_stacked = np.repeat(img_adapted[...,None],3,axis=2)\n",
    "        resized = cv2.resize(img_rgb, (224,224), interpolation=cv2.INTER_AREA)\n",
    "        bg_img = 255 * np.ones(shape=(224,224,3))\n",
    "#         print(bg_img.shape,resized.shape)\n",
    "        bg_img[0:224, 0:224,:] = resized\n",
    "        bg_img /= 255\n",
    "        bg_img = np.rollaxis(bg_img, 2, 0)  \n",
    "#         print(bg_img.shape)\n",
    "        return bg_img\n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r',encoding='UTF-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, data_dir, input_transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_filenames = []\n",
    "        self.texts = []\n",
    "        all_filenames = listdir(data_dir)\n",
    "        all_filenames.sort()\n",
    "        for filename in (all_filenames):\n",
    "            if filename[-3:] == \"png\":\n",
    "                self.image_filenames.append(filename)\n",
    "            else:\n",
    "                text = '<START> ' + load_doc(self.data_dir+filename) + ' <END>'\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.replace(',', ' ,')\n",
    "                self.texts.append(text)\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Initialize the function to create the vocabulary \n",
    "        tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "        # Create the vocabulary \n",
    "        tokenizer.fit_on_texts([load_doc('vocabulary.vocab')])\n",
    "        self.tokenizer = tokenizer\n",
    "        # Add one spot for the empty word in the vocabulary \n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        # Map the input sentences into the vocabulary indexes\n",
    "        self.train_sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "        # The longest set of boostrap tokens\n",
    "        self.max_sequence = max(len(s) for s in self.train_sequences)\n",
    "        # Specify how many tokens to have in each input sentence\n",
    "        self.max_length = 48\n",
    "        \n",
    "        X, y, image_data_filenames = list(), list(), list()\n",
    "        for img_no, seq in enumerate(self.train_sequences):\n",
    "            print(img_no)\n",
    "            in_seq, out_seq = seq[:-1], seq[1:]\n",
    "            out_seq = to_categorical(out_seq, num_classes=self.vocab_size)\n",
    "            image_data_filenames.append(self.image_filenames[img_no])\n",
    "            X.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            print(\"->\",out_seq)\n",
    "                \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.image_data_filenames = image_data_filenames\n",
    "        self.images = list()\n",
    "        for image_name in self.image_data_filenames:\n",
    "            image = resize_img(self.data_dir+image_name)\n",
    "            self.images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "1\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "2\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "3\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "4\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "5\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "6\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "7\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "8\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "9\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "10\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "11\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "12\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "13\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "14\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "15\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "16\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "17\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "18\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "19\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "20\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "21\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "22\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "23\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "24\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "25\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "26\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "27\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "28\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "29\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "30\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "31\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "32\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "33\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "34\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "35\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "36\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "37\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "38\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "39\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "40\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "41\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "42\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "43\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "44\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "45\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "46\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "47\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "48\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "49\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'all_data5/'\n",
    "batch_size = 32\n",
    "my_dateset = Dataset(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(my_dateset.images,dtype=np.float32)\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i]=np.array(x_train[i],dtype=np.float32)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (?, 3, 224, 224)\n",
      "aft (?, 224, 224, 3)\n",
      "0\n",
      "******** <tf.Variable 'W0:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "1\n",
      "conv1: (?, 224, 224, 32)\n",
      "2\n",
      "conv2: (?, 224, 224, 32)\n",
      "3\n",
      "maxpool: (?, 112, 112, 32)\n",
      "okay\n",
      "conv3: (?, 112, 112, 64)\n",
      "conv3: (?, 112, 112, 64)\n",
      "maxpool: (?, 56, 56, 64)\n",
      "conv5: (?, 56, 56, 128)\n",
      "conv6: (?, 56, 56, 128)\n",
      "conv6: (?, 28, 28, 128)\n",
      "fc1: (?, 1024)\n",
      "WARNING:tensorflow:From <ipython-input-8-239044277af1>:71: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "fc2: (?, 1024)\n",
      "Tensor(\"dropout_1/mul_1:0\", shape=(?, 1024), dtype=float32)\n",
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (?, 3, 224, 224)\n",
      "aft (?, 224, 224, 3)\n",
      "0\n",
      "******** <tf.Variable 'W0:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "1\n",
      "conv1: (?, 224, 224, 32)\n",
      "2\n",
      "conv2: (?, 224, 224, 32)\n",
      "3\n",
      "maxpool: (?, 112, 112, 32)\n",
      "okay\n",
      "conv3: (?, 112, 112, 64)\n",
      "conv3: (?, 112, 112, 64)\n",
      "maxpool: (?, 56, 56, 64)\n",
      "conv5: (?, 56, 56, 128)\n",
      "conv6: (?, 56, 56, 128)\n",
      "conv6: (?, 28, 28, 128)\n",
      "fc1: (?, 1024)\n",
      "fc2: (?, 1024)\n",
      "Tensor(\"Relu_15:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "im = tf.placeholder(dtype=tf.float32, shape=(None,3,224,224), name='im')\n",
    "# is_training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "model_train = cnn_train(im,weights,biases)\n",
    "model_test = cnn_test(im,weights,biases)\n",
    "output_train = batch_norm_wrapper(model_train,True)\n",
    "output_test = batch_norm_wrapper(model_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "expected = my_dateset.y\n",
    "expected=np.array(expected)\n",
    "for e in range(len(expected)):\n",
    "    expected[e]=np.array(expected[e])\n",
    "print(expected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_t: (?, 256)\n",
      "h_t: (?, 256)\n",
      "h_t: (?, 256)\n",
      "h_t: (?, 512)\n",
      "h_t: (?, 512)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_LEN=19\n",
    "EMBED_SIZE=50\n",
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_LEN, EMBED_SIZE]))\n",
    "caption_p = tf.placeholder(dtype=tf.int32, shape=(None,None), name='caption_p')\n",
    "embed = tf.nn.embedding_lookup(embeddings, caption_p)\n",
    "\n",
    "gru_before = GRU(50,256,embed)\n",
    "gru_before_1 = GRU(256,256,gru_before.h_t)\n",
    "gru_before_2 = GRU(256,256,gru_before_1.h_t)\n",
    "# gru_after =  GRU(50,256,)\n",
    "# hidden_size=256\n",
    "\n",
    "\n",
    "\n",
    "# W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 18), mean=0, stddev=0.01),trainable=True)\n",
    "# b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(18,), mean=0, stddev=0.01),trainable=True)\n",
    "\n",
    "Wout_gru1 = weights['Wout_gru1']\n",
    "bout_gru1 = biases['Bout_gru1']\n",
    "\n",
    "# output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "#output_gru1 = tf.nn.softmax(tf.matmul(gru_before_1.h_t,Wout_gru1)+bout_gru1)\n",
    "output_gru1 = gru_before_2.h_t\n",
    "#gru_before_1 = GRU(256,256,gru_before.h_t)\n",
    "# out2 = tf.matmul(gru.h_t[0], W_output)+b_output\n",
    "\n",
    "# tf.get_variable('W7', shape=(1024,50), initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# out3 = gru.h_t\n",
    "# out4 = gru_final.h_t\n",
    "# print(out3.shape)\n",
    "# print(out4.shape)\n",
    "\n",
    "features_try = K.tile(K.expand_dims(output_train, 1), [1, K.shape(output_gru1)[1], 1])\n",
    "embeddings = tf.concat([features_try,output_gru1],2)\n",
    "\n",
    "\n",
    "gru_final = GRU(1280,512,embeddings)\n",
    "gru_final1 = GRU(512,512,gru_final.h_t)\n",
    "Wout_gru2 = weights['Wout_gru2']\n",
    "bout_gru2 = biases['Bout_gru2']\n",
    "\n",
    "output_gru2 = tf.nn.softmax(tf.matmul(gru_final1.h_t,Wout_gru2)+bout_gru2)\n",
    "\n",
    "true_output = tf.placeholder(dtype=tf.float32, shape=(None,None,None), name='expected_output')\n",
    "loss = tf.reduce_sum(tf.squared_difference(output_gru2 ,true_output)) #/ float(1)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "def pad(batch_y):\n",
    "    print(batch_y.shape)\n",
    "    x=0\n",
    "    for y in batch_y:\n",
    "        if(len(y)>x):\n",
    "            x=len(y)\n",
    "#     x = functools.reduce(lambda x,y: len(x) if(len(x)>len(y)) else len(y),batch_y)\n",
    "    \n",
    "    ret = []\n",
    "    for y in range(len(batch_y)):\n",
    "        res=np.zeros(x)\n",
    "        s = batch_y[y]\n",
    "        res[0:len(s)]=batch_y[y]\n",
    "#         batch_y[y]=res\n",
    "        ret.append(res)\n",
    "    return np.array(ret)\n",
    "        \n",
    "        \n",
    "# a=[[1,2],[1,2,3]]\n",
    "# pad(a)\n",
    "\n",
    "def pad2(batch_ex):\n",
    "#     r = functools.reduce(lambda x,y: len(x) if(len(x)>len(y)) else len(y),batch_ex)\n",
    "#     print(\":::::\",r)\n",
    "    r=0\n",
    "    c=0\n",
    "    for ex in batch_ex:\n",
    "        shape = ex.shape\n",
    "#         print(shape)\n",
    "        if(shape[0]>r):\n",
    "            r=shape[0]\n",
    "        if(shape[1]>c):\n",
    "            c=shape[1]\n",
    "#     c = functools.reduce(lambda x,y: len(x[0]) if(len(x[0])>len(y[0])) else len(y[0]),batch_ex)\n",
    "#     print(\":::::\",c)\n",
    "#     print(r,c)\n",
    "    ret=[]\n",
    "    for ex in batch_ex:\n",
    "        res=np.zeros((r,c))\n",
    "#         print(res.shape)\n",
    "#         print(ex.shape)\n",
    "        res[0:ex.shape[0],0:ex.shape[1]]=ex\n",
    "        ret.append(res)\n",
    "#     print(ret)\n",
    "        \n",
    "    return(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.1732421875\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.90379638671875\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "72.77974853515624\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.05892944335938\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.02548828125\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.476513671875\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.08052978515625\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.82356567382813\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.48351440429687\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.8421875\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "1\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.83395385742188\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.50953979492188\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "72.40059204101563\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.65201416015626\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.68262939453125\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.10771484375\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.6862060546875\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.3533447265625\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.76026611328125\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.2215087890625\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "2\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.21996459960937\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.31873168945313\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "70.97940673828126\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.9512939453125\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.49967041015626\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.030389404296876\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.94859008789062\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.8805419921875\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.912542724609374\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.174005126953126\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "3\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.1662353515625\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.87874145507813\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "67.8188720703125\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.71121215820312\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.72347412109375\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.07454833984375\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.74238891601563\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.1744873046875\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.56587524414063\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.33018188476562\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "4\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.67811889648438\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.84360961914062\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "67.58055419921875\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.33269653320312\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.658380126953126\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.21478271484375\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.93101806640625\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.59775390625\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.12813110351563\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.519891357421876\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "5\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.57564086914063\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.1336181640625\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "68.1191650390625\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.5878173828125\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.42698364257812\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.909637451171875\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.8724609375\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.13580322265625\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.26143188476563\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.133837890625\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "6\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.010980224609376\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.58489990234375\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "67.20379028320312\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.38927001953125\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.23857421875\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.699658203125\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.601348876953125\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.66312866210937\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.222747802734375\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.12199096679687\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "7\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.2572265625\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.56004638671875\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "67.36717529296875\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.2293701171875\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.325341796875\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.5394287109375\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.4154296875\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.112603759765626\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.05891723632813\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.016693115234375\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "8\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.12413330078125\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.54232788085938\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "67.36553344726562\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.07368774414063\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.213232421875\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.42112426757812\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.13417358398438\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.2145263671875\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.7528076171875\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "54.86944580078125\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "9\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.900537109375\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.37637939453126\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5, 76, 18)\n",
      "(5, 76)\n",
      "67.06292724609375\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.92490844726562\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.03089599609375\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.51181640625\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.06279296875\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.01500244140625\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.8005615234375\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "54.789031982421875\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "vocab_size = 19\n",
    "batch_size=5\n",
    "\n",
    "x_train = my_dateset.images\n",
    "caption = my_dateset.X\n",
    "expected = my_dateset.y\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    loss_ar=[]\n",
    "    for e in range(epoch):\n",
    "        loss_no=[]\n",
    "        print(e)\n",
    "        for batch in range(len(x_train)//batch_size):\n",
    "            print(\"batch \",batch)\n",
    "            batch_x = x_train[batch*batch_size:min((batch+1)*batch_size,len(x_train))]\n",
    "            batch_y = caption[batch*batch_size:min((batch+1)*batch_size,len(caption))] \n",
    "            batch_ex = expected[batch*batch_size:min((batch+1)*batch_size,len(expected))]\n",
    "            print(\"x:\",len(batch_x))\n",
    "            print(\"y:\",len(batch_y))\n",
    "            print(\"ex:\",len(batch_ex))\n",
    "            \n",
    "#             print(batch_y)\n",
    "            \n",
    "            batch_x = np.array(batch_x)\n",
    "            for b in range(len(batch_x)):\n",
    "                batch_x[b]=np.array(batch_x[b])\n",
    "            batch_y = np.array(batch_y)\n",
    "            for b in range(len(batch_y)):\n",
    "                batch_y[b]=np.array(batch_y[b])\n",
    "            batch_ex = np.array(batch_ex)\n",
    "            for b in range(len(batch_ex)):\n",
    "                batch_ex[b]=np.array(batch_ex[b])\n",
    "                \n",
    "            print(\"bex:\",batch_ex.shape)\n",
    "#             print(batch_ex[0].shape)\n",
    "                \n",
    "            batch_y = pad(batch_y)\n",
    "            batch_ex = pad2(batch_ex)\n",
    "            \n",
    "#             batch_y = batch_y.reshape((-1,1))\n",
    "#             print(batch_ex.shape)\n",
    "            \n",
    "\n",
    "            # print(\"Sssss:\",ex.shape)\n",
    "            ls,tr = sess.run([loss,train_step],feed_dict ={true_output:batch_ex,im:batch_x,caption_p:batch_y})\n",
    "            print(ls/batch_size)\n",
    "            loss_no.append(ls/batch_size)\n",
    "            print(\"\\n\\n\")\n",
    "#         el = sess.run(epoch_loss,feed_dict={e_loss:loss_no})\n",
    "        loss_ar.append(loss_no)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------\") \n",
    "    save_path = saver.save(sess, \"model10.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.10164794921874, 70.7837890625, 72.65057983398438, 69.0483154296875, 68.02144775390624, 59.3773193359375, 60.08030395507812, 59.93126220703125, 66.38382568359376, 58.739404296875]\n",
      "[64.68706665039062, 70.42816162109375, 72.26708984375, 68.67350463867187, 67.59902954101562, 59.01786499023437, 59.640618896484376, 59.4095947265625, 65.73562622070312, 58.142486572265625]\n",
      "[63.845166015625, 69.33342895507812, 70.88839721679688, 67.02454833984375, 65.625341796875, 56.98030395507813, 57.1434814453125, 56.63511962890625, 62.52720336914062, 55.86893310546875]\n",
      "[62.57744140625, 65.89756469726562, 67.52384033203126, 64.46629638671875, 63.6174560546875, 55.89561157226562, 56.618951416015626, 56.52298583984375, 62.33046264648438, 55.424078369140624]\n",
      "[61.548492431640625, 65.78167114257812, 67.4919677734375, 64.28148803710937, 63.52486572265625, 55.77720336914062, 56.64241333007813, 55.99527587890625, 62.04964599609375, 54.925048828125]\n",
      "[61.91465454101562, 65.87943115234376, 67.52506103515626, 64.39519653320312, 63.2427978515625, 55.740576171875, 56.281243896484376, 56.1275634765625, 61.928167724609374, 55.205535888671875]\n",
      "[61.58055419921875, 65.63822021484376, 67.2948974609375, 64.25343627929688, 63.39287109375, 55.49041748046875, 56.3614013671875, 56.01130981445313, 61.854913330078126, 54.8168701171875]\n",
      "[61.290777587890624, 65.54796142578125, 67.33743286132812, 64.0516357421875, 63.1590576171875, 55.41943359375, 56.157666015625, 56.49320068359375, 61.79569091796875, 54.928863525390625]\n",
      "[60.920068359375, 65.5909912109375, 67.21856689453125, 63.9601318359375, 62.987738037109374, 55.43284912109375, 56.199896240234374, 55.81973876953125, 61.743463134765626, 54.730865478515625]\n",
      "[60.736962890625, 65.38945922851562, 67.04591064453125, 63.94337768554688, 62.94636840820313, 55.224493408203124, 55.96736450195313, 55.78626708984375, 61.673370361328125, 54.770782470703125]\n"
     ]
    }
   ],
   "source": [
    "for l in loss_ar:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    print(tokenizer.word_index.items())\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def load_val_images(data_dir):\n",
    "    image_filenames =[]\n",
    "    images = []\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if filename[-3:] == \"png\":\n",
    "            image_filenames.append(filename)\n",
    "    for name in image_filenames:\n",
    "        image = resize_img(data_dir+name)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_words = []\n",
    "star_text = '<START> '\n",
    "image = load_val_images('all_data5/')[35]\n",
    "img_tensor=np.expand_dims(np.array(image),0)\n",
    "img_tensor=np.array(img_tensor)\n",
    "predicted = '<START> '\n",
    " \n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model10.ckpt\n",
      "(1, 1)\n",
      "[0.034906674, 0.049482975, 0.0544656, 0.060248356, 0.053772617, 0.043561935, 0.059138093, 0.08920273, 0.052030053, 0.049945958, 0.09046771, 0.060585875, 0.04154859, 0.08634229, 0.04624667, 0.038249373, 0.04322531, 0.04657915]\n",
      "10\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green \n",
      "(1, 2)\n",
      "[0.03263107, 0.059608594, 0.062464274, 0.06666814, 0.05749256, 0.04773391, 0.0603321, 0.08682609, 0.053770997, 0.049617108, 0.08508026, 0.059800625, 0.042393193, 0.0702998, 0.04185858, 0.037389137, 0.042016998, 0.04401664]\n",
      "7\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row \n",
      "(1, 3)\n",
      "[0.029071793, 0.0759285, 0.07473437, 0.07550583, 0.062529005, 0.05361103, 0.061482836, 0.08289807, 0.054646406, 0.048625514, 0.07752823, 0.05823144, 0.04289213, 0.05165188, 0.035500806, 0.035465628, 0.039527826, 0.040168617]\n",
      "7\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row \n",
      "(1, 4)\n",
      "[0.02456464, 0.09928701, 0.091118485, 0.08556755, 0.06792356, 0.060385905, 0.061736256, 0.07707728, 0.053983033, 0.04651004, 0.06802428, 0.055301633, 0.042522665, 0.03461508, 0.028211512, 0.03233615, 0.0356955, 0.035139434]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , \n",
      "(1, 5)\n",
      "[0.01993238, 0.12759022, 0.10946713, 0.095103346, 0.07260218, 0.06686389, 0.060673643, 0.0698251, 0.051477004, 0.043414053, 0.058025282, 0.051198747, 0.04108284, 0.022031622, 0.021447008, 0.028428754, 0.031107625, 0.02972918]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , \n",
      "(1, 6)\n",
      "[0.015982827, 0.15563804, 0.1263193, 0.103056036, 0.07618925, 0.07229818, 0.05875034, 0.062403154, 0.047670733, 0.040086966, 0.049321346, 0.046845157, 0.038958672, 0.014109986, 0.016190063, 0.0245481, 0.026699869, 0.024931861]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , \n",
      "(1, 7)\n",
      "[0.013059232, 0.17822324, 0.13902104, 0.10957196, 0.07906064, 0.07672654, 0.056823574, 0.05600286, 0.043557823, 0.03726775, 0.042857476, 0.043162286, 0.036793888, 0.009554342, 0.0125830015, 0.021316282, 0.023126576, 0.021291384]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , \n",
      "(1, 8)\n",
      "[0.011072346, 0.19316067, 0.14694215, 0.11528786, 0.081698656, 0.080520794, 0.055454757, 0.051107075, 0.039881043, 0.035255022, 0.038569707, 0.040527444, 0.035008725, 0.0070005152, 0.01026778, 0.018912792, 0.020530298, 0.018802283]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , \n",
      "(1, 9)\n",
      "[0.009775503, 0.20122908, 0.15088445, 0.12060348, 0.08429549, 0.083919354, 0.054753594, 0.047609825, 0.036908876, 0.033981636, 0.03593331, 0.03886146, 0.033703536, 0.005545955, 0.008816006, 0.017228302, 0.01874996, 0.017200192]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , \n",
      "(1, 10)\n",
      "[0.008937097, 0.20444445, 0.15208203, 0.1255962, 0.086811475, 0.086972974, 0.054578777, 0.045182936, 0.03461792, 0.033241265, 0.034399886, 0.03790991, 0.032806434, 0.004687543, 0.007903207, 0.016071944, 0.017554741, 0.016201086]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , \n",
      "(1, 11)\n",
      "[0.008392342, 0.20473792, 0.1516221, 0.13019177, 0.089142844, 0.08965487, 0.054738857, 0.043508023, 0.03288407, 0.032840032, 0.033554565, 0.037426647, 0.03220598, 0.0041604405, 0.007320971, 0.015277208, 0.01675298, 0.015588315]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , \n",
      "(1, 12)\n",
      "[0.008032821, 0.20352563, 0.15029159, 0.13430515, 0.09120851, 0.091943905, 0.055076618, 0.042339776, 0.031574987, 0.032637652, 0.0331186, 0.037224185, 0.03180631, 0.0038238233, 0.006942139, 0.014723914, 0.016209142, 0.015215215]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , \n",
      "(1, 13)\n",
      "[0.0077912062, 0.20168458, 0.14858721, 0.13789888, 0.09298067, 0.09385002, 0.055484753, 0.041513614, 0.03058242, 0.03254412, 0.03291684, 0.037181377, 0.03153807, 0.0036012037, 0.0066904295, 0.014331591, 0.015834166, 0.01498886]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , \n",
      "(1, 14)\n",
      "[0.007626631, 0.19968012, 0.14679696, 0.14097646, 0.09446401, 0.09540469, 0.055904448, 0.04092399, 0.029827202, 0.03251004, 0.03285147, 0.0372235, 0.031360053, 0.0034504295, 0.0065213316, 0.014050248, 0.015573371, 0.014855072]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , \n",
      "(1, 15)\n",
      "[0.0075111683, 0.1977942, 0.14508265, 0.14357515, 0.095686324, 0.09665737, 0.056300946, 0.040493015, 0.029247286, 0.032501787, 0.032853343, 0.03730393, 0.031239105, 0.0033445624, 0.006404302, 0.013843126, 0.015387078, 0.014774613]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , \n",
      "(1, 16)\n",
      "[0.0074295285, 0.19610375, 0.14351913, 0.14574803, 0.09668293, 0.09765697, 0.056659076, 0.04017556, 0.028800108, 0.03250706, 0.032893386, 0.037399735, 0.031159958, 0.0032693432, 0.00632287, 0.013689689, 0.015254161, 0.014728678]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , \n",
      "(1, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0073694857, 0.19466347, 0.14214297, 0.14755307, 0.09749106, 0.098453134, 0.056973018, 0.039935946, 0.028450934, 0.032515626, 0.03294427, 0.037492804, 0.031105649, 0.0032136843, 0.006264204, 0.013573069, 0.015155595, 0.014701847]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , \n",
      "(1, 18)\n",
      "[0.007325175, 0.19345048, 0.14094214, 0.14904535, 0.09814691, 0.09908399, 0.0572461, 0.039755564, 0.028176686, 0.032524724, 0.033000514, 0.037581477, 0.03107127, 0.0031726554, 0.0062221973, 0.013484178, 0.01508355, 0.014687153]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , \n",
      "(1, 19)\n",
      "[0.0072919186, 0.1924437, 0.13991545, 0.15027541, 0.09867451, 0.09958357, 0.057477515, 0.039617546, 0.027962284, 0.03253236, 0.03305534, 0.03765992, 0.031050524, 0.0031415643, 0.0061913836, 0.013415805, 0.01503051, 0.014680536]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , \n",
      "(1, 20)\n",
      "[0.0072660847, 0.19162352, 0.13903959, 0.15128966, 0.09910114, 0.09997984, 0.057674654, 0.039510418, 0.027791398, 0.03253723, 0.033104695, 0.03772823, 0.031038111, 0.0031175432, 0.0061681475, 0.013362285, 0.01498997, 0.01467724]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , \n",
      "(1, 21)\n",
      "[0.0072459723, 0.19094655, 0.13830149, 0.15212335, 0.09944643, 0.10029567, 0.057841983, 0.039427962, 0.027655624, 0.032541282, 0.03314864, 0.037787005, 0.031032028, 0.003098792, 0.006150694, 0.013320137, 0.0149594555, 0.014676901]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , \n",
      "(1, 22)\n",
      "[0.0072298474, 0.19040157, 0.1376776, 0.15280911, 0.099725716, 0.100546554, 0.05798211, 0.03936303, 0.027546458, 0.03254316, 0.03318586, 0.03783731, 0.031030316, 0.0030840626, 0.006137314, 0.013286681, 0.014935956, 0.014677401]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , \n",
      "(1, 23)\n",
      "[0.0072174664, 0.18995182, 0.13714877, 0.15337083, 0.09995238, 0.100747235, 0.05810106, 0.03931335, 0.027459448, 0.032545608, 0.03322009, 0.037881088, 0.031031527, 0.0030726462, 0.006127493, 0.013260855, 0.0149189895, 0.014679501]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 24)\n",
      "[0.007206687, 0.18959081, 0.13671009, 0.15383826, 0.10013685, 0.10090875, 0.05819997, 0.039272264, 0.027387943, 0.032544367, 0.033246905, 0.037916258, 0.031034295, 0.0030630506, 0.006119102, 0.013239141, 0.014904779, 0.014680554]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 25)\n",
      "[0.0071979626, 0.18929991, 0.1363403, 0.15422337, 0.10028639, 0.10103912, 0.058282472, 0.03923816, 0.027329825, 0.032543495, 0.03327039, 0.03794568, 0.031037787, 0.003055268, 0.006112546, 0.013221837, 0.01489378, 0.014681763]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 26)\n",
      "[0.0071917, 0.1890454, 0.13602403, 0.15453842, 0.10041069, 0.10114343, 0.058354516, 0.039215654, 0.02728384, 0.032544214, 0.033293236, 0.037973378, 0.03104309, 0.0030497417, 0.006108247, 0.013209177, 0.014886719, 0.014684659]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 27)\n",
      "[0.0071859765, 0.1888502, 0.13576064, 0.15479809, 0.10051053, 0.101226464, 0.058415268, 0.039195813, 0.02724616, 0.032543026, 0.03331038, 0.03799534, 0.031048112, 0.0030447512, 0.0061044106, 0.013198045, 0.014880612, 0.0146862725]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 28)\n",
      "[0.00718018, 0.18869516, 0.13554493, 0.15501936, 0.10059462, 0.10129913, 0.058463283, 0.03917624, 0.027212813, 0.032539804, 0.033322763, 0.038011216, 0.03105146, 0.0030400332, 0.006100319, 0.013188034, 0.014874444, 0.01468628]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 29)\n",
      "[0.007176302, 0.1885625, 0.13535675, 0.15519974, 0.100663595, 0.101354934, 0.05850577, 0.039163277, 0.02718698, 0.032539036, 0.03333515, 0.038026754, 0.031055696, 0.003036624, 0.006097767, 0.013180503, 0.014870593, 0.014687906]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 30)\n",
      "[0.007173029, 0.1884488, 0.13520046, 0.15534858, 0.10071811, 0.10140269, 0.05854183, 0.039153207, 0.027166178, 0.03253809, 0.03334621, 0.03804033, 0.031060623, 0.0030339845, 0.0060958294, 0.013174786, 0.014867879, 0.01468928]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 31)\n",
      "[0.0071702586, 0.18835847, 0.13506924, 0.15547259, 0.10076461, 0.10143993, 0.05857238, 0.039144274, 0.027148886, 0.032536883, 0.033355575, 0.038050663, 0.031064445, 0.0030317246, 0.006094163, 0.013169934, 0.014865693, 0.014690368]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007167753, 0.18828166, 0.13495855, 0.15557781, 0.10080379, 0.10147119, 0.058597255, 0.03913738, 0.027133947, 0.032535873, 0.03336294, 0.038060356, 0.031068059, 0.003029792, 0.0060927775, 0.013165963, 0.014863697, 0.014691192]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 33)\n",
      "[0.007165548, 0.1882238, 0.13486497, 0.15566336, 0.10083514, 0.10149867, 0.058619585, 0.039131224, 0.027121264, 0.03253462, 0.033368506, 0.03806766, 0.031070841, 0.0030280317, 0.006091313, 0.013162256, 0.014861844, 0.014691545]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 34)\n",
      "[0.0071641435, 0.18816783, 0.13478665, 0.15573445, 0.100860275, 0.10151888, 0.058637805, 0.039127175, 0.027112117, 0.032533944, 0.033375155, 0.038075283, 0.031074632, 0.0030268559, 0.006090659, 0.013159917, 0.014861392, 0.0146928085]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 35)\n",
      "[0.007162347, 0.18813032, 0.13472122, 0.15579511, 0.10088267, 0.10153788, 0.058652166, 0.03912233, 0.027102817, 0.032532256, 0.03337916, 0.03807978, 0.03107698, 0.0030255236, 0.0060895626, 0.013157331, 0.014859871, 0.014692763]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 36)\n",
      "[0.0071610254, 0.1880948, 0.13466485, 0.15584606, 0.10090129, 0.10155196, 0.05866577, 0.03911904, 0.027095966, 0.032531224, 0.0333829, 0.038084537, 0.031079125, 0.0030245862, 0.0060889176, 0.0131554175, 0.014859421, 0.014693201]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 37)\n",
      "[0.0071600177, 0.188066, 0.13461795, 0.1558882, 0.10091501, 0.10156404, 0.058676742, 0.03911654, 0.02709036, 0.032530926, 0.0333855, 0.038088914, 0.031081308, 0.0030238521, 0.006088497, 0.013153715, 0.014858745, 0.01469375]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 38)\n",
      "[0.0071593937, 0.18803832, 0.13457389, 0.1559221, 0.100928016, 0.10157357, 0.058687016, 0.039116096, 0.02708575, 0.032531276, 0.03338982, 0.038092755, 0.031083757, 0.0030234158, 0.006088294, 0.0131530985, 0.014858924, 0.014694561]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 39)\n",
      "[0.007158686, 0.18801717, 0.13454165, 0.15595174, 0.10093791, 0.10158227, 0.058694597, 0.03911414, 0.027082149, 0.0325306, 0.033392355, 0.038095627, 0.031085385, 0.0030228854, 0.0060878764, 0.01315192, 0.014858419, 0.014694789]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 40)\n",
      "[0.0071576852, 0.1880058, 0.13451374, 0.155976, 0.100946724, 0.10159097, 0.058700725, 0.03911155, 0.027077995, 0.032529045, 0.03339331, 0.038097866, 0.031086046, 0.0030221867, 0.0060872617, 0.013150614, 0.014858037, 0.0146945175]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 41)\n",
      "[0.007157307, 0.18798758, 0.13448718, 0.15599796, 0.100954644, 0.101596, 0.058707584, 0.039110605, 0.02707532, 0.03252926, 0.033395983, 0.038100094, 0.031088011, 0.0030219285, 0.0060872985, 0.013150349, 0.014857908, 0.014694951]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 42)\n",
      "[0.0071568633, 0.18797399, 0.13446759, 0.15601593, 0.10096144, 0.10160054, 0.058712427, 0.039110586, 0.027072709, 0.032528386, 0.03339781, 0.03810199, 0.031088615, 0.00302158, 0.00608703, 0.01314958, 0.014857719, 0.014695191]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 43)\n",
      "[0.007156365, 0.1879636, 0.1344512, 0.15603143, 0.10096553, 0.10160485, 0.058716983, 0.039108932, 0.027070817, 0.03252792, 0.03339887, 0.038103625, 0.031089466, 0.0030212433, 0.0060868124, 0.013149239, 0.014857641, 0.014695388]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 44)\n",
      "[0.0071561867, 0.18795687, 0.13443542, 0.15604006, 0.10096989, 0.10160886, 0.05871996, 0.039108653, 0.027069084, 0.032529134, 0.033400435, 0.038104676, 0.031090502, 0.0030211466, 0.006086664, 0.013148927, 0.014857644, 0.014695905]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 45)\n",
      "[0.007155564, 0.18795037, 0.13442366, 0.15605369, 0.10097409, 0.101612054, 0.05872395, 0.03910786, 0.027067589, 0.032528087, 0.03339978, 0.03810523, 0.031090619, 0.0030207152, 0.006086271, 0.013148082, 0.014856999, 0.014695331]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0071553034, 0.18794674, 0.13441278, 0.15606353, 0.10097643, 0.10161566, 0.05872568, 0.03910636, 0.027066436, 0.032526586, 0.033401057, 0.038105983, 0.031090831, 0.003020419, 0.006086089, 0.013147659, 0.014856982, 0.0146952495]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 47)\n",
      "[0.007155185, 0.18794148, 0.13440377, 0.1560696, 0.10097846, 0.10161598, 0.058727834, 0.039106455, 0.027065312, 0.032526713, 0.03340217, 0.03810752, 0.031092182, 0.0030204512, 0.006086226, 0.013147472, 0.014857405, 0.014695715]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 48)\n",
      "[0.0071554687, 0.18793038, 0.1343947, 0.15607394, 0.10098151, 0.101618014, 0.058730725, 0.03910765, 0.027065517, 0.032527987, 0.033403408, 0.038108516, 0.031092757, 0.0030206458, 0.0060865683, 0.013148075, 0.01485787, 0.014696497]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 49)\n",
      "[0.0071548037, 0.18793523, 0.13438979, 0.1560812, 0.100982256, 0.101619594, 0.058731243, 0.039105568, 0.02706356, 0.03252641, 0.033403043, 0.03810857, 0.03109277, 0.003020194, 0.0060860305, 0.013147117, 0.014856978, 0.0146956565]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "(1, 50)\n",
      "[0.007154799, 0.18792732, 0.13438426, 0.15608673, 0.100984246, 0.10162028, 0.058733158, 0.039106447, 0.02706321, 0.032527007, 0.0334037, 0.038109053, 0.031093074, 0.0030201962, 0.006086035, 0.013147312, 0.014857172, 0.014696045]\n",
      "1\n",
      "dict_items([(',', 1), ('{', 2), ('}', 3), ('small-title', 4), ('text', 5), ('quadruple', 6), ('row', 7), ('btn-inactive', 8), ('btn-orange', 9), ('btn-green', 10), ('btn-red', 11), ('double', 12), ('<START>', 13), ('header', 14), ('btn-active', 15), ('<END>', 16), ('single', 17)])\n",
      "<START>btn-green row row , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "features_try = K.tile(K.expand_dims(output_test, 1), [1, K.shape(output_gru1)[1], 1])\n",
    "embeddings = tf.concat([features_try,output_gru1],2)\n",
    "\n",
    "predicted='<START>'\n",
    "star_text = '<START>'\n",
    "with tf.Session() as sess:\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "    saver.restore(sess, \"model10.ckpt\")\n",
    "#     print(\"####\",weights['W_conv1'].eval())\n",
    "    for di in range(50):\n",
    "        #print(star_text)\n",
    "        sequence = my_dateset.tokenizer.texts_to_sequences([star_text])\n",
    "#         decoder_input = to_categorical(sequence, num_classes=18)\n",
    "#         print(decoder_input)\n",
    "#         print(sequence)\n",
    "        decoder_input = np.array(sequence).reshape(-1,1)\n",
    "       # print(decoder_input)\n",
    "        temp =[]\n",
    "        for x in sequence:\n",
    "            temp.append(x)\n",
    "        \n",
    "        temp = np.array(temp)\n",
    "        print(temp.shape)\n",
    "    \n",
    "        a = sess.run(output_gru2, feed_dict={im:img_tensor,caption_p:temp})\n",
    "        #print(a)\n",
    "        \n",
    "        data=list(a[0][-1])\n",
    "        print(data)\n",
    "        i=data.index(max(data))\n",
    "        print(i)\n",
    "        word = word_for_id(i,my_dateset.tokenizer)\n",
    "        #print(word)\n",
    "        if word is None:\n",
    "#             print(x)\n",
    "            continue\n",
    "        predicted += word + ' '\n",
    "        star_text += ' ' +word\n",
    "        print(predicted)\n",
    "        if word == '<END>':\n",
    "            pass\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
