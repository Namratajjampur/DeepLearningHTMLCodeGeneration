{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "    \n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dimensions, hidden_size, inputs,dtype=tf.float32):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_layer=[]\n",
    "        \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = inputs\n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float32, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "#         print(self.x_t.shape)\n",
    "        \n",
    "        print(\"h_t:\",h_t.shape)\n",
    "#         print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "\n",
    "    def __init__(self, input_nodes, hidden_unit, inputs,dtype=tf.float32):\n",
    "\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_unit = hidden_unit\n",
    "#         self.output_nodes = output_nodes\n",
    "\n",
    "        self.Wi = tf.Variable(tf.zeros([self.input_nodes, self.hidden_unit]))\n",
    "        self.Ui = tf.Variable(tf.zeros([self.hidden_unit, self.hidden_unit]))\n",
    "        self.bi = tf.Variable(tf.zeros([self.hidden_unit]))\n",
    "\n",
    "        self.Wf = tf.Variable(tf.zeros([self.input_nodes, self.hidden_unit]))\n",
    "        self.Uf = tf.Variable(tf.zeros([self.hidden_unit, self.hidden_unit]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_unit]))\n",
    "\n",
    "        self.Wog = tf.Variable(tf.zeros([self.input_nodes, self.hidden_unit]))\n",
    "        self.Uog = tf.Variable(tf.zeros([self.hidden_unit, self.hidden_unit]))\n",
    "        self.bog = tf.Variable(tf.zeros([self.hidden_unit]))\n",
    "\n",
    "        self.Wc = tf.Variable(tf.zeros([self.input_nodes, self.hidden_unit]))\n",
    "        self.Uc = tf.Variable(tf.zeros([self.hidden_unit, self.hidden_unit]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_unit]))\n",
    "\n",
    "        # Weights for output layers\n",
    "#         self.Wo = tf.Variable(tf.truncated_normal([self.hidden_unit, self.output_nodes], mean=0, stddev=.01))\n",
    "#         self.bo = tf.Variable(tf.truncated_normal([self.output_nodes], mean=0, stddev=.01))\n",
    "\n",
    "        # Placeholder for input vector with shape[batch, seq, embeddings]\n",
    "#         self.inputs = tf.placeholder(tf.float32, shape=[None, None, self.input_nodes], name='inputs')\n",
    "        self.input_layer = inputs\n",
    "    \n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        self.initial_hidden = tf.matmul(self.x_t[0,:,:], tf.zeros(dtype=tf.float32, shape=(self.input_nodes, self.hidden_unit)), name='h_0')\n",
    "\n",
    "\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        # Process tensor of size [5,3,2] to [3,5,2]\n",
    "#         batch_input_ = tf.transpose(self.inputs, perm=[1, 0, 2])\n",
    "#         self.processed_input = tf.transpose(batch_input_)\n",
    "#         self.processed_input = self.inputs\n",
    "\n",
    "#         self.initial_hidden = tf.matmul(self.inputs[0, :, :], tf.zeros(dtype=tf.float32, shape=(self.input_nodes, self.hidden_unit)), name='h_0')\n",
    "\n",
    "#         self.initial_hidden = self.inputs[:, 0, :]\n",
    "#         self.initial_hidden = tf.matmul(self.initial_hidden, tf.zeros([input_nodes, hidden_unit]))\n",
    "\n",
    "#         self.initial_hidden = tf.stack([self.initial_hidden, self.initial_hidden])\n",
    "        self.c_t = self.initial_hidden\n",
    "        print(self.x_t.shape,self.initial_hidden.shape)\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.initial_hidden, name='h_t_transposed')\n",
    "\n",
    "#         self.h_tt,self.c_t = tf.unstack(self.h_t_transposed)\n",
    "        \n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "#         self.h_t = self.h_t_transposed\n",
    "\n",
    "\n",
    "\n",
    "    def forward_pass(self, previous_hidden_memory_tuple, x):\n",
    "        # Take previous hidden stats and memory tuple with i/p &\n",
    "        # o/p current hidden state\n",
    "\n",
    "#         previous_hidden_state, c_prev = tf.unstack(previous_hidden_memory_tuple)\n",
    "        previous_hidden_state = previous_hidden_memory_tuple\n",
    "        c_prev = self.c_t\n",
    "\n",
    "        i = tf.sigmoid( tf.matmul(x, self.Wi) +\n",
    "                        tf.matmul(previous_hidden_state, self.Ui) + self.bi)\n",
    "\n",
    "        f = tf.sigmoid( tf.matmul(x, self.Wf) +\n",
    "                        tf.matmul(previous_hidden_state, self.Uf) + self.bf)\n",
    "\n",
    "        o = tf.sigmoid( tf.matmul(x, self.Wog) +\n",
    "                        tf.matmul(previous_hidden_state, self.Uog) + self.bog)\n",
    "\n",
    "        c_ = tf.nn.tanh(tf.matmul(x, self.Wc) +\n",
    "                        tf.matmul(previous_hidden_state, self.Uc) + self.bc)\n",
    "\n",
    "        # Final Memory cell\n",
    "        c = f * c_prev + i * c_\n",
    "        current_hidden_state = o * tf.nn.tanh(c)\n",
    "        \n",
    "        self.h_t = current_hidden_state \n",
    "#         self.h_t = tf.transpose(self.h_t, [1, 0, 2], name='h_t')\n",
    "        \n",
    "        self.c_t = c\n",
    "\n",
    "        return current_hidden_state\n",
    "    \n",
    "#     @tf.function\n",
    "#     def get_states(self):\n",
    "#         all_hidden_states = tf.scan(self.Lstm, self.processed_input, initializer=self.initial_hidden, name='states')\n",
    "#         all_hidden_states = all_hidden_states[:, 0, :, :]\n",
    "#         return all_hidden_states\n",
    "\n",
    "#     def get_output(self, hidden_state):\n",
    "#         output = tf.nn.relu(tf.matmul(hidden_state, self.Wo) + self.bo)\n",
    "#         return output\n",
    "\n",
    "#     def get_outputs(self):\n",
    "#         all_hidden_states = self.get_states()\n",
    "#         all_outputs = tf.map_fn(self.get_output, all_hidden_states)\n",
    "#         return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vb_size=18\n",
    "batchsize=2\n",
    "weights = {\n",
    "    'W_conv1': tf.get_variable('W0', shape=(3,3,3,32), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv2': tf.get_variable('W1', shape=(3,3,32,32), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv3': tf.get_variable('W2', shape=(3,3,32,64), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv4': tf.get_variable('W3', shape=(3,3,64,64), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv5': tf.get_variable('W4', shape=(3,3,64,128), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_conv6': tf.get_variable('W5', shape=(3,3,128,128), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_fc1': tf.get_variable('W6', shape=(28*28*128,1024), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'W_fc2': tf.get_variable('W7', shape=(1024,1024), initializer=tf.contrib.layers.variance_scaling_initializer()), \n",
    "    'Wout_gru1': tf.get_variable('W8', dtype = tf.float32,shape=(256,256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Wout_gru_1': tf.get_variable('W10', dtype = tf.float32,shape=(256,256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Wout_gru2': tf.get_variable('W9', dtype = tf.float32,shape=(512,vb_size), initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    }\n",
    "biases = {\n",
    "    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc2': tf.get_variable('B1', shape=(32), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc3': tf.get_variable('B2', shape=(64), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc4': tf.get_variable('B3', shape=(64), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc5': tf.get_variable('B4', shape=(128), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'bc6': tf.get_variable('B5', shape=(128), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'b_fc1': tf.get_variable('B6', shape=(1024), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'b_fc2': tf.get_variable('B7', shape=(1024), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Bout_gru1':tf.get_variable('B8', dtype = tf.float32,shape=(256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Bout_gru_1':tf.get_variable('B10', dtype = tf.float32,shape=(256), initializer=tf.contrib.layers.variance_scaling_initializer()),\n",
    "    'Bout_gru2': tf.get_variable('B9', dtype = tf.float32,shape=(vb_size), initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn_test(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    \n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"********\",weights['W_conv1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "#    conv2 = tf.nn.dropout(conv2, 0.25)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    "#    conv4 = tf.nn.dropout(conv4, 0.25)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "#    conv6 = tf.nn.dropout(conv6, 0.25)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "#    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "#    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "#     x_norm = batch_norm_wrapper(fc2,is_training)\n",
    "    \n",
    "    \n",
    "#     inputs=fc2\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     decay=0.9999\n",
    "\n",
    "#     if is_training:\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "#     else:\n",
    "#         return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "#     x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "#     print(x_norm.shape)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn_train(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    \n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"********\",weights['W_conv1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "#    conv2 = tf.nn.dropout(conv2, 0.2)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    " #   conv4 = tf.nn.dropout(conv4, 0.2)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    " #   conv6 = tf.nn.dropout(conv6, 0.2)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "#     x_norm = batch_norm_wrapper(fc2,is_training)\n",
    "    \n",
    "    \n",
    "#     inputs=fc2\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     decay=0.9999\n",
    "\n",
    "#     if is_training:\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "#     else:\n",
    "#         return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "#     x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "#     print(x_norm.shape)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "import cv2\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def resize_img(png_file_path):\n",
    "        img_rgb = cv2.imread(png_file_path)\n",
    "        #img_grey = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "        #img_adapted = cv2.adaptiveThreshold(img_grey, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY, 101, 9)\n",
    "        #img_stacked = np.repeat(img_adapted[...,None],3,axis=2)\n",
    "        resized = cv2.resize(img_rgb, (224,224), interpolation=cv2.INTER_AREA)\n",
    "        bg_img = 255 * np.ones(shape=(224,224,3))\n",
    "#         print(bg_img.shape,resized.shape)\n",
    "        bg_img[0:224, 0:224,:] = resized\n",
    "        bg_img /= 255\n",
    "        bg_img = np.rollaxis(bg_img, 2, 0)  \n",
    "#         print(bg_img.shape)\n",
    "        return bg_img\n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r',encoding='UTF-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, data_dir, input_transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_filenames = []\n",
    "        self.texts = []\n",
    "        all_filenames = listdir(data_dir)\n",
    "        all_filenames.sort()\n",
    "        for filename in (all_filenames):\n",
    "            if filename[-3:] == \"png\":\n",
    "                self.image_filenames.append(filename)\n",
    "            else:\n",
    "                text = '<START> ' + load_doc(self.data_dir+filename) + ' <END>'\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.replace(',', ' ,')\n",
    "                self.texts.append(text)\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Initialize the function to create the vocabulary \n",
    "        tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "        # Create the vocabulary \n",
    "        tokenizer.fit_on_texts([load_doc('vocabulary.vocab')])\n",
    "        self.tokenizer = tokenizer\n",
    "        # Add one spot for the empty word in the vocabulary \n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        # Map the input sentences into the vocabulary indexes\n",
    "        self.train_sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "        # The longest set of boostrap tokens\n",
    "        self.max_sequence = max(len(s) for s in self.train_sequences)\n",
    "        # Specify how many tokens to have in each input sentence\n",
    "        self.max_length = 48\n",
    "        \n",
    "        X, y, image_data_filenames = list(), list(), list()\n",
    "        for img_no, seq in enumerate(self.train_sequences):\n",
    "            print(img_no)\n",
    "            in_seq, out_seq = seq[:-1], seq[1:]\n",
    "            out_seq = to_categorical(out_seq, num_classes=self.vocab_size)\n",
    "            image_data_filenames.append(self.image_filenames[img_no])\n",
    "            X.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            print(\"->\",out_seq)\n",
    "                \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.image_data_filenames = image_data_filenames\n",
    "        self.images = list()\n",
    "        for image_name in self.image_data_filenames:\n",
    "            image = resize_img(self.data_dir+image_name)\n",
    "            self.images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "1\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "2\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "3\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "4\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "5\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "6\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "7\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "8\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "9\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "10\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "11\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "12\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "13\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "14\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "15\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "16\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "17\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "18\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "19\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "20\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "21\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "22\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "23\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "24\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "25\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "26\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "27\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "28\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "29\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "30\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "31\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "32\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "33\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "34\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "35\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "36\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "37\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "38\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "39\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "40\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "41\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "42\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "43\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "44\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "45\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "46\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "47\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "48\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "49\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "50\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "51\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "52\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "53\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "54\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "55\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "56\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "57\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "58\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "59\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "61\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "62\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "63\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "64\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "65\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "66\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "67\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "68\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "69\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "70\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "71\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "72\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "73\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "74\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "75\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "76\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "77\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "78\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "79\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "80\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "81\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "82\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "83\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "84\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "85\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "86\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "87\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "88\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "89\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "90\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "91\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "92\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "93\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "94\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "95\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "96\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "97\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "98\n",
      "-> [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "99\n",
      "-> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'all_data5/'\n",
    "batch_size = 32\n",
    "my_dateset = Dataset(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(my_dateset.images,dtype=np.float32)\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i]=np.array(x_train[i],dtype=np.float32)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (?, 3, 224, 224)\n",
      "aft (?, 224, 224, 3)\n",
      "0\n",
      "******** <tf.Variable 'W0:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "1\n",
      "conv1: (?, 224, 224, 32)\n",
      "2\n",
      "conv2: (?, 224, 224, 32)\n",
      "3\n",
      "maxpool: (?, 112, 112, 32)\n",
      "okay\n",
      "conv3: (?, 112, 112, 64)\n",
      "conv3: (?, 112, 112, 64)\n",
      "maxpool: (?, 56, 56, 64)\n",
      "conv5: (?, 56, 56, 128)\n",
      "conv6: (?, 56, 56, 128)\n",
      "conv6: (?, 28, 28, 128)\n",
      "fc1: (?, 1024)\n",
      "WARNING:tensorflow:From <ipython-input-9-239044277af1>:71: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "fc2: (?, 1024)\n",
      "Tensor(\"dropout_1/mul_1:0\", shape=(?, 1024), dtype=float32)\n",
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (?, 3, 224, 224)\n",
      "aft (?, 224, 224, 3)\n",
      "0\n",
      "******** <tf.Variable 'W0:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "1\n",
      "conv1: (?, 224, 224, 32)\n",
      "2\n",
      "conv2: (?, 224, 224, 32)\n",
      "3\n",
      "maxpool: (?, 112, 112, 32)\n",
      "okay\n",
      "conv3: (?, 112, 112, 64)\n",
      "conv3: (?, 112, 112, 64)\n",
      "maxpool: (?, 56, 56, 64)\n",
      "conv5: (?, 56, 56, 128)\n",
      "conv6: (?, 56, 56, 128)\n",
      "conv6: (?, 28, 28, 128)\n",
      "fc1: (?, 1024)\n",
      "fc2: (?, 1024)\n",
      "Tensor(\"Relu_15:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "im = tf.placeholder(dtype=tf.float32, shape=(None,3,224,224), name='im')\n",
    "# is_training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "model_train = cnn_train(im,weights,biases)\n",
    "model_test = cnn_test(im,weights,biases)\n",
    "output_train = batch_norm_wrapper(model_train,True)\n",
    "output_test = batch_norm_wrapper(model_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "expected = my_dateset.y\n",
    "expected=np.array(expected)\n",
    "for e in range(len(expected)):\n",
    "    expected[e]=np.array(expected[e])\n",
    "print(expected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 50) (?, 256)\n",
      "(?, ?, 256) (?, 256)\n",
      "(?, ?, 1280) (?, 512)\n",
      "(?, ?, 512) (?, 512)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_LEN=19\n",
    "EMBED_SIZE=50\n",
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_LEN, EMBED_SIZE]))\n",
    "caption_p = tf.placeholder(dtype=tf.int32, shape=(None,None), name='caption_p')\n",
    "embed = tf.nn.embedding_lookup(embeddings, caption_p)\n",
    "\n",
    "gru_before = LSTM(50,256,embed)\n",
    "# tf.function(gru_before.get_states)\n",
    "gru_before_1 = LSTM(256,256,gru_before.h_t)\n",
    "# tf.function(gru_before_1.get_states)\n",
    "# gru_before_2 = LSTM_cell(256,256,gru_before_1.h_t)\n",
    "# gru_after =  GRU(50,256,)\n",
    "# hidden_size=256\n",
    "\n",
    "\n",
    "\n",
    "# W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 18), mean=0, stddev=0.01),trainable=True)\n",
    "# b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(18,), mean=0, stddev=0.01),trainable=True)\n",
    "\n",
    "Wout_gru1 = weights['Wout_gru1']\n",
    "bout_gru1 = biases['Bout_gru1']\n",
    "\n",
    "# output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "#output_gru1 = tf.nn.softmax(tf.matmul(gru_before_1.h_t,Wout_gru1)+bout_gru1)\n",
    "output_gru1 = gru_before_1.h_t\n",
    "#gru_before_1 = GRU(256,256,gru_before.h_t)\n",
    "# out2 = tf.matmul(gru.h_t[0], W_output)+b_output\n",
    "\n",
    "# tf.get_variable('W7', shape=(1024,50), initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# out3 = gru.h_t\n",
    "# out4 = gru_final.h_t\n",
    "# print(out3.shape)\n",
    "# print(out4.shape)\n",
    "\n",
    "features_try = K.tile(K.expand_dims(output_train, 1), [1, K.shape(output_gru1)[1], 1])\n",
    "embeddings = tf.concat([features_try,output_gru1],2)\n",
    "\n",
    "\n",
    "gru_final = LSTM(1280,512,embeddings)\n",
    "# tf.function(gru_final.get_states)\n",
    "gru_final1 = LSTM(512,512,gru_final.h_t)\n",
    "# tf.function(gru_final1.get_states)\n",
    "Wout_gru2 = weights['Wout_gru2']\n",
    "bout_gru2 = biases['Bout_gru2']\n",
    "\n",
    "output_gru2 = tf.nn.softmax(tf.matmul(gru_final1.h_t,Wout_gru2)+bout_gru2)\n",
    "\n",
    "true_output = tf.placeholder(dtype=tf.float32, shape=(None,None,None), name='expected_output')\n",
    "loss = tf.reduce_sum(tf.squared_difference(output_gru2 ,true_output)) #/ float(1)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "def pad(batch_y):\n",
    "    print(batch_y.shape)\n",
    "    x=0\n",
    "    for y in batch_y:\n",
    "        if(len(y)>x):\n",
    "            x=len(y)\n",
    "#     x = functools.reduce(lambda x,y: len(x) if(len(x)>len(y)) else len(y),batch_y)\n",
    "    \n",
    "    ret = []\n",
    "    for y in range(len(batch_y)):\n",
    "        res=np.zeros(x)\n",
    "        s = batch_y[y]\n",
    "        res[0:len(s)]=batch_y[y]\n",
    "#         batch_y[y]=res\n",
    "        ret.append(res)\n",
    "    return np.array(ret)\n",
    "        \n",
    "        \n",
    "# a=[[1,2],[1,2,3]]\n",
    "# pad(a)\n",
    "\n",
    "def pad2(batch_ex):\n",
    "#     r = functools.reduce(lambda x,y: len(x) if(len(x)>len(y)) else len(y),batch_ex)\n",
    "#     print(\":::::\",r)\n",
    "    r=0\n",
    "    c=0\n",
    "    for ex in batch_ex:\n",
    "        shape = ex.shape\n",
    "#         print(shape)\n",
    "        if(shape[0]>r):\n",
    "            r=shape[0]\n",
    "        if(shape[1]>c):\n",
    "            c=shape[1]\n",
    "#     c = functools.reduce(lambda x,y: len(x[0]) if(len(x[0])>len(y[0])) else len(y[0]),batch_ex)\n",
    "#     print(\":::::\",c)\n",
    "#     print(r,c)\n",
    "    ret=[]\n",
    "    for ex in batch_ex:\n",
    "        res=np.zeros((r,c))\n",
    "#         print(res.shape)\n",
    "#         print(ex.shape)\n",
    "        res[0:ex.shape[0],0:ex.shape[1]]=ex\n",
    "        ret.append(res)\n",
    "#     print(ret)\n",
    "        \n",
    "    return(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.349462890625\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "72.22408447265624\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.0339599609375\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.09205322265625\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.25008544921874\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.139111328125\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.89408569335937\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.88258666992188\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.36072998046875\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.23052978515625\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.2354736328125\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.16724243164063\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.56812744140625\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.77046508789063\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.47587890625\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "48.82741088867188\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "71.75685424804688\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.0383056640625\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.2808349609375\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "71.65196533203125\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "1\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.2917236328125\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "72.156103515625\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.973321533203126\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.02249755859376\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.17854614257813\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.06642456054688\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.82320556640625\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.811767578125\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.28829345703125\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.14461669921874\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.155810546875\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.07617797851563\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.48170166015625\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.6593994140625\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.35880737304687\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "48.7444580078125\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "71.62157592773437\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.89835815429687\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.1367919921875\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "71.48829345703125\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "2\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.1353271484375\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "71.9632568359375\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.794256591796874\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.8067138671875\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.9456298828125\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.81790161132812\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.56917724609375\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "59.544097900390625\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.996826171875\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.76895751953126\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.7833984375\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.60787353515625\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.99985961914062\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.94241943359376\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.4915771484375\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "48.10321044921875\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.14265747070313\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.80555419921875\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.4406982421875\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.0634521484375\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "3\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.02042236328125\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.70736694335938\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.841522216796875\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.79235229492187\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.01396484375\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.97001953125\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.842431640625\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.91334228515625\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.42235107421875\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.14530029296876\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.30423583984375\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.141510009765625\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.674017333984374\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.635693359375\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.34181518554688\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "48.135748291015624\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.60990600585937\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.9036376953125\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.18512573242188\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.5116455078125\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "4\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "63.27940063476562\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.92820434570312\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.938958740234376\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.83748779296874\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.99547119140625\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.9030029296875\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.741241455078125\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.79007568359375\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.2720947265625\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.93167114257812\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.09867553710937\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.88028564453125\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.434716796875\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.2781494140625\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.94892578125\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "47.921435546875\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.1700439453125\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.41629028320312\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.68516845703125\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.000439453125\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "5\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.799755859375\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.2197021484375\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.3276123046875\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.02772216796875\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.13178100585938\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.98688354492188\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.855126953125\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.984197998046874\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.484765625\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.99771728515626\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.35284423828125\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.8647705078125\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.78829345703125\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.076318359375\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.78338623046875\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "47.61575622558594\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.41328125\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.42876586914062\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.79095458984375\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.00218505859375\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "6\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.37718505859375\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.72721557617187\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.089361572265624\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.904150390625\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.14942626953125\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.13352661132812\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.07547607421875\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "58.21495361328125\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.71497802734375\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.24422607421874\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.5364013671875\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.229150390625\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.90675048828125\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.50999145507812\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.14417724609375\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "47.56368713378906\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.38682250976562\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.522021484375\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.7839599609375\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.34339599609375\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "7\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.21998291015625\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.0760009765625\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.46488647460937\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.82573852539062\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.18712158203125\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.65240478515625\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.40899658203125\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.671923828125\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.1364990234375\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.47281494140626\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.01881103515625\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.64378662109375\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.5232177734375\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.8896240234375\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.53231811523438\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "47.41536865234375\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.98231201171875\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.001904296875\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.3217529296875\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.74232177734375\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "8\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.07630615234375\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.00183715820313\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.50728759765625\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.1361083984375\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.3809814453125\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.37446899414063\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.36729736328125\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.59910888671875\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.068109130859376\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.3066162109375\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.803515625\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.29093017578125\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.33670654296875\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.2359130859375\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.78529052734375\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "47.9186767578125\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "70.12489013671875\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "66.6659423828125\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.29429931640625\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.0438232421875\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "9\n",
      "batch  0\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "62.0437744140625\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "69.30255737304688\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.7220703125\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.46322021484374\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.69901123046876\n",
      "\n",
      "\n",
      "\n",
      "batch  5\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "65.693603515625\n",
      "\n",
      "\n",
      "\n",
      "batch  6\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "60.66163940429688\n",
      "\n",
      "\n",
      "\n",
      "batch  7\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.84574584960937\n",
      "\n",
      "\n",
      "\n",
      "batch  8\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "56.3483154296875\n",
      "\n",
      "\n",
      "\n",
      "batch  9\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.79256591796874\n",
      "\n",
      "\n",
      "\n",
      "batch  10\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "57.14927978515625\n",
      "\n",
      "\n",
      "\n",
      "batch  11\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "61.79046630859375\n",
      "\n",
      "\n",
      "\n",
      "batch  12\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "55.54822998046875\n",
      "\n",
      "\n",
      "\n",
      "batch  13\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.03126831054688\n",
      "\n",
      "\n",
      "\n",
      "batch  14\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.66622924804688\n",
      "\n",
      "\n",
      "\n",
      "batch  15\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "47.32423706054688\n",
      "\n",
      "\n",
      "\n",
      "batch  16\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.96109008789062\n",
      "\n",
      "\n",
      "\n",
      "batch  17\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "67.09530029296874\n",
      "\n",
      "\n",
      "\n",
      "batch  18\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "64.387060546875\n",
      "\n",
      "\n",
      "\n",
      "batch  19\n",
      "x: 5\n",
      "y: 5\n",
      "ex: 5\n",
      "bex: (5,)\n",
      "(5,)\n",
      "68.92296142578125\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "time: 1723.498298883438\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "vocab_size = 19\n",
    "batch_size=5\n",
    "\n",
    "x_train = my_dateset.images\n",
    "caption = my_dateset.X\n",
    "expected = my_dateset.y\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    loss_ar=[]\n",
    "    t1 = time.time()\n",
    "    for e in range(epoch):\n",
    "        loss_no=[]\n",
    "        print(e)\n",
    "        for batch in range(len(x_train)//batch_size):\n",
    "            print(\"batch \",batch)\n",
    "            batch_x = x_train[batch*batch_size:min((batch+1)*batch_size,len(x_train))]\n",
    "            batch_y = caption[batch*batch_size:min((batch+1)*batch_size,len(caption))] \n",
    "            batch_ex = expected[batch*batch_size:min((batch+1)*batch_size,len(expected))]\n",
    "            print(\"x:\",len(batch_x))\n",
    "            print(\"y:\",len(batch_y))\n",
    "            print(\"ex:\",len(batch_ex))\n",
    "            \n",
    "#             print(batch_y)\n",
    "            \n",
    "            batch_x = np.array(batch_x)\n",
    "            for b in range(len(batch_x)):\n",
    "                batch_x[b]=np.array(batch_x[b])\n",
    "            batch_y = np.array(batch_y)\n",
    "            for b in range(len(batch_y)):\n",
    "                batch_y[b]=np.array(batch_y[b])\n",
    "            batch_ex = np.array(batch_ex)\n",
    "            for b in range(len(batch_ex)):\n",
    "                batch_ex[b]=np.array(batch_ex[b])\n",
    "                \n",
    "            print(\"bex:\",batch_ex.shape)\n",
    "#             print(batch_ex[0].shape)\n",
    "                \n",
    "            batch_y = pad(batch_y)\n",
    "            batch_ex = pad2(batch_ex)\n",
    "            \n",
    "#             batch_y = batch_y.reshape((-1,1))\n",
    "#             print(batch_ex.shape)\n",
    "            \n",
    "\n",
    "            # print(\"Sssss:\",ex.shape)\n",
    "            ls,tr = sess.run([loss,train_step],feed_dict ={true_output:batch_ex,im:batch_x,caption_p:batch_y})\n",
    "            print(ls/batch_size)\n",
    "            loss_no.append(ls/batch_size)\n",
    "            print(\"\\n\\n\")\n",
    "#         el = sess.run(epoch_loss,feed_dict={e_loss:loss_no})\n",
    "        loss_ar.append(loss_no)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------\") \n",
    "    t2 = time.time()\n",
    "    save_path = saver.save(sess, \"model10.ckpt\")\n",
    "print(\"\\n\\n\\n\\n\\n\\ntime:\",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64.349462890625, 72.22408447265624, 63.0339599609375, 70.09205322265625, 68.25008544921874, 68.139111328125, 62.89408569335937, 59.88258666992188, 58.36072998046875, 67.23052978515625, 59.2354736328125, 64.16724243164063, 57.56812744140625, 70.77046508789063, 70.47587890625, 48.82741088867188, 71.75685424804688, 70.0383056640625, 67.2808349609375, 71.65196533203125]\n",
      "[64.2917236328125, 72.156103515625, 62.973321533203126, 70.02249755859376, 68.17854614257813, 68.06642456054688, 62.82320556640625, 59.811767578125, 58.28829345703125, 67.14461669921874, 59.155810546875, 64.07617797851563, 57.48170166015625, 70.6593994140625, 70.35880737304687, 48.7444580078125, 71.62157592773437, 69.89835815429687, 67.1367919921875, 71.48829345703125]\n",
      "[64.1353271484375, 71.9632568359375, 62.794256591796874, 69.8067138671875, 67.9456298828125, 67.81790161132812, 62.56917724609375, 59.544097900390625, 57.996826171875, 66.76895751953126, 58.7833984375, 63.60787353515625, 56.99985961914062, 69.94241943359376, 69.4915771484375, 48.10321044921875, 70.14265747070313, 68.80555419921875, 65.4406982421875, 70.0634521484375]\n",
      "[63.02042236328125, 70.70736694335938, 61.841522216796875, 68.79235229492187, 67.01396484375, 66.97001953125, 61.842431640625, 58.91334228515625, 57.42235107421875, 66.14530029296876, 58.30423583984375, 63.141510009765625, 56.674017333984374, 69.635693359375, 69.34181518554688, 48.135748291015624, 70.60990600585937, 68.9036376953125, 66.18512573242188, 70.5116455078125]\n",
      "[63.27940063476562, 70.92820434570312, 61.938958740234376, 68.83748779296874, 66.99547119140625, 66.9030029296875, 61.741241455078125, 58.79007568359375, 57.2720947265625, 65.93167114257812, 58.09867553710937, 62.88028564453125, 56.434716796875, 69.2781494140625, 68.94892578125, 47.921435546875, 70.1700439453125, 68.41629028320312, 65.68516845703125, 70.000439453125]\n",
      "[62.799755859375, 70.2197021484375, 61.3276123046875, 68.02772216796875, 66.13178100585938, 65.98688354492188, 60.855126953125, 57.984197998046874, 56.484765625, 64.99771728515626, 57.35284423828125, 61.8647705078125, 55.78829345703125, 68.076318359375, 67.78338623046875, 47.61575622558594, 69.41328125, 67.42876586914062, 64.79095458984375, 70.00218505859375]\n",
      "[62.37718505859375, 69.72721557617187, 61.089361572265624, 67.904150390625, 66.14942626953125, 66.13352661132812, 61.07547607421875, 58.21495361328125, 56.71497802734375, 65.24422607421874, 57.5364013671875, 62.229150390625, 55.90675048828125, 68.50999145507812, 68.14417724609375, 47.56368713378906, 69.38682250976562, 67.522021484375, 64.7839599609375, 69.34339599609375]\n",
      "[62.21998291015625, 69.0760009765625, 60.46488647460937, 66.82573852539062, 65.18712158203125, 65.65240478515625, 60.40899658203125, 57.671923828125, 56.1364990234375, 64.47281494140626, 57.01881103515625, 61.64378662109375, 55.5232177734375, 67.8896240234375, 67.53231811523438, 47.41536865234375, 68.98231201171875, 67.001904296875, 64.3217529296875, 69.74232177734375]\n",
      "[62.07630615234375, 69.00183715820313, 60.50728759765625, 67.1361083984375, 65.3809814453125, 65.37446899414063, 60.36729736328125, 57.59910888671875, 56.068109130859376, 64.3066162109375, 56.803515625, 61.29093017578125, 55.33670654296875, 67.2359130859375, 66.78529052734375, 47.9186767578125, 70.12489013671875, 66.6659423828125, 64.29429931640625, 69.0438232421875]\n",
      "[62.0437744140625, 69.30255737304688, 60.7220703125, 67.46322021484374, 65.69901123046876, 65.693603515625, 60.66163940429688, 57.84574584960937, 56.3483154296875, 64.79256591796874, 57.14927978515625, 61.79046630859375, 55.54822998046875, 68.03126831054688, 67.66622924804688, 47.32423706054688, 68.96109008789062, 67.09530029296874, 64.387060546875, 68.92296142578125]\n"
     ]
    }
   ],
   "source": [
    "for l in loss_ar:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    print(tokenizer.word_index.items())\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def load_val_images(data_dir):\n",
    "    image_filenames =[]\n",
    "    images = []\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if filename[-3:] == \"png\":\n",
    "            image_filenames.append(filename)\n",
    "    for name in image_filenames:\n",
    "        image = resize_img(data_dir+name)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_words = []\n",
    "star_text = '<START> '\n",
    "image = load_val_images('all_data5/')[35]\n",
    "img_tensor=np.expand_dims(np.array(image),0)\n",
    "img_tensor=np.array(img_tensor)\n",
    "predicted = '<START> '\n",
    " \n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model10.ckpt\n"
     ]
    }
   ],
   "source": [
    "features_try = K.tile(K.expand_dims(output_test, 1), [1, K.shape(output_gru1)[1], 1])\n",
    "embeddings = tf.concat([features_try,output_gru1],2)\n",
    "\n",
    "predicted='<START>'\n",
    "star_text = '<START>'\n",
    "with tf.Session() as sess:\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "    saver.restore(sess, \"model10.ckpt\")\n",
    "#     print(\"####\",weights['W_conv1'].eval())\n",
    "    for di in range(50):\n",
    "        #print(star_text)\n",
    "        sequence = my_dateset.tokenizer.texts_to_sequences([star_text])\n",
    "#         decoder_input = to_categorical(sequence, num_classes=18)\n",
    "#         print(decoder_input)\n",
    "#         print(sequence)\n",
    "        decoder_input = np.array(sequence).reshape(-1,1)\n",
    "       # print(decoder_input)\n",
    "        temp =[]\n",
    "        for x in sequence:\n",
    "            temp.append(x)\n",
    "        \n",
    "        temp = np.array(temp)\n",
    "        print(temp.shape)\n",
    "    \n",
    "        a = sess.run(output_gru2, feed_dict={im:img_tensor,caption_p:temp})\n",
    "        #print(a)\n",
    "        \n",
    "        data=list(a[0][-1])\n",
    "        print(data)\n",
    "        i=data.index(max(data))\n",
    "        print(i)\n",
    "        word = word_for_id(i,my_dateset.tokenizer)\n",
    "        #print(word)\n",
    "        if word is None:\n",
    "#             print(x)\n",
    "            continue\n",
    "        predicted += word + ' '\n",
    "        star_text += ' ' +word\n",
    "        print(predicted)\n",
    "        if word == '<END>':\n",
    "            pass\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
