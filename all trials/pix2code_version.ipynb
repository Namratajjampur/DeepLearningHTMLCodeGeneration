{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size=512\n",
    "vb_size=18\n",
    "batchsize=2\n",
    "weights = {\n",
    "    'W_conv1': tf.get_variable('W0', shape=(3,3,3,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv2': tf.get_variable('W1', shape=(3,3,32,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv3': tf.get_variable('W2', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv4': tf.get_variable('W3', shape=(3,3,64,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv5': tf.get_variable('W4', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv6': tf.get_variable('W5', shape=(3,3,128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_fc1': tf.get_variable('W6', shape=(28*28*128,1024), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_fc2': tf.get_variable('W7', shape=(1024,1024), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'out': tf.get_variable('W8', dtype = tf.float64,shape=(hidden_size,vb_size), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    }\n",
    "biases = {\n",
    "    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc2': tf.get_variable('B1', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc3': tf.get_variable('B2', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc4': tf.get_variable('B3', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc5': tf.get_variable('B4', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc6': tf.get_variable('B5', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'b_fc1': tf.get_variable('B6', shape=(1024), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'b_fc2': tf.get_variable('B7', shape=(1024), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'out': tf.get_variable('B8', dtype = tf.float64,shape=(vb_size), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "    \n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_layer=[]\n",
    "        \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "#         print(self.x_t.shape)\n",
    "        \n",
    "        print(\"h_t:\",h_t.shape)\n",
    "#         print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        \n",
    "        return h_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn_test(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    \n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"********\",weights['W_conv1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "#    conv2 = tf.nn.dropout(conv2, 0.25)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    "#    conv4 = tf.nn.dropout(conv4, 0.25)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "#    conv6 = tf.nn.dropout(conv6, 0.25)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "#    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "#    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "#     x_norm = batch_norm_wrapper(fc2,is_training)\n",
    "    \n",
    "    \n",
    "#     inputs=fc2\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     decay=0.9999\n",
    "\n",
    "#     if is_training:\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "#     else:\n",
    "#         return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "#     x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "#     print(x_norm.shape)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn_train(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    \n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"********\",weights['W_conv1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "    conv2 = tf.nn.dropout(conv2, 0.25)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    "    conv4 = tf.nn.dropout(conv4, 0.25)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    conv6 = tf.nn.dropout(conv6, 0.25)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "#     x_norm = batch_norm_wrapper(fc2,is_training)\n",
    "    \n",
    "    \n",
    "#     inputs=fc2\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     decay=0.9999\n",
    "\n",
    "#     if is_training:\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.nn.batch_normalization(inputs,batch_mean, batch_var, beta, scale, epsilon)\n",
    "#     else:\n",
    "#         return tf.nn.batch_normalization(inputs,pop_mean, pop_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "#     x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "#     print(x_norm.shape)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "import cv2\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def resize_img(png_file_path):\n",
    "        img_rgb = cv2.imread(png_file_path)\n",
    "        img_grey = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "        img_adapted = cv2.adaptiveThreshold(img_grey, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY, 101, 9)\n",
    "        img_stacked = np.repeat(img_adapted[...,None],3,axis=2)\n",
    "        resized = cv2.resize(img_stacked, (224,224), interpolation=cv2.INTER_AREA)\n",
    "        bg_img = 255 * np.ones(shape=(224,224,3))\n",
    "#         print(bg_img.shape,resized.shape)\n",
    "        bg_img[0:224, 0:224,:] = resized\n",
    "        bg_img /= 255\n",
    "        bg_img = np.rollaxis(bg_img, 2, 0)  \n",
    "#         print(bg_img.shape)\n",
    "        return bg_img\n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r',encoding='UTF-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, data_dir, input_transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_filenames = []\n",
    "        self.texts = []\n",
    "        all_filenames = listdir(data_dir)\n",
    "        all_filenames.sort()\n",
    "        for filename in (all_filenames):\n",
    "            if filename[-3:] == \"png\":\n",
    "                self.image_filenames.append(filename)\n",
    "            else:\n",
    "                text = '<START> ' + load_doc(self.data_dir+filename) + ' <END>'\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.replace(',', ' ,')\n",
    "                self.texts.append(text)\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Initialize the function to create the vocabulary \n",
    "        tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "        # Create the vocabulary \n",
    "        tokenizer.fit_on_texts([load_doc('vocabulary.vocab')])\n",
    "        self.tokenizer = tokenizer\n",
    "        # Add one spot for the empty word in the vocabulary \n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        # Map the input sentences into the vocabulary indexes\n",
    "        self.train_sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "        # The longest set of boostrap tokens\n",
    "        self.max_sequence = max(len(s) for s in self.train_sequences)\n",
    "        # Specify how many tokens to have in each input sentence\n",
    "        self.max_length = 48\n",
    "        \n",
    "        X, y, image_data_filenames = list(), list(), list()\n",
    "        for img_no, seq in enumerate(self.train_sequences):\n",
    "            in_seq, out_seq = seq[:-1], seq[1:]\n",
    "            out_seq = to_categorical(out_seq, num_classes=self.vocab_size)\n",
    "            image_data_filenames.append(self.image_filenames[img_no])\n",
    "            X.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "                \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.image_data_filenames = image_data_filenames\n",
    "        self.images = list()\n",
    "        for image_name in self.image_data_filenames:\n",
    "            image = resize_img(self.data_dir+image_name)\n",
    "            self.images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'all_data5/'\n",
    "batch_size = 32\n",
    "my_dateset = Dataset(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(my_dateset.images,dtype=np.float32)\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i]=np.array(x_train[i],dtype=np.float32)\n",
    "print(x_train.shape)\n",
    "# batch_size = 128\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x))\n",
    "# iterator = dataset.repeat().batch(batch_size).make_initializable_iterator()\n",
    "# data_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (?, 3, 224, 224)\n",
      "aft (?, 224, 224, 3)\n",
      "0\n",
      "******** <tf.Variable 'W0:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "1\n",
      "conv1: (?, 224, 224, 32)\n",
      "2\n",
      "conv2: (?, 224, 224, 32)\n",
      "3\n",
      "maxpool: (?, 112, 112, 32)\n",
      "WARNING:tensorflow:From <ipython-input-8-e1bf793897e6>:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "okay\n",
      "conv3: (?, 112, 112, 64)\n",
      "conv3: (?, 112, 112, 64)\n",
      "maxpool: (?, 56, 56, 64)\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "conv5: (?, 56, 56, 128)\n",
      "conv6: (?, 56, 56, 128)\n",
      "conv6: (?, 28, 28, 128)\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "fc1: (?, 1024)\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "fc2: (?, 1024)\n",
      "Tensor(\"dropout_4/mul_1:0\", shape=(?, 1024), dtype=float32)\n",
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (?, 3, 224, 224)\n",
      "aft (?, 224, 224, 3)\n",
      "0\n",
      "******** <tf.Variable 'W0:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "1\n",
      "conv1: (?, 224, 224, 32)\n",
      "2\n",
      "conv2: (?, 224, 224, 32)\n",
      "3\n",
      "maxpool: (?, 112, 112, 32)\n",
      "okay\n",
      "conv3: (?, 112, 112, 64)\n",
      "conv3: (?, 112, 112, 64)\n",
      "maxpool: (?, 56, 56, 64)\n",
      "conv5: (?, 56, 56, 128)\n",
      "conv6: (?, 56, 56, 128)\n",
      "conv6: (?, 28, 28, 128)\n",
      "fc1: (?, 1024)\n",
      "fc2: (?, 1024)\n",
      "Tensor(\"Relu_15:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "im = tf.placeholder(dtype=tf.float32, shape=(None,3,224,224), name='im')\n",
    "# is_training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "model_train = cnn_train(im,weights,biases)\n",
    "model_test = cnn_test(im,weights,biases)\n",
    "output_train = batch_norm_wrapper(model_train,True)\n",
    "output_test = batch_norm_wrapper(model_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_t: (?, 256)\n",
      "h_t: (?, 256)\n",
      "h_t: (?, 256)\n",
      "h_t: (?, 512)\n",
      "(?, ?, 256)\n",
      "(?, ?, 512)\n"
     ]
    }
   ],
   "source": [
    "gru_before = GRU(50,256)\n",
    "gru_after =  GRU(50,256)\n",
    "gru = GRU(50,256)\n",
    "hidden_size=256\n",
    "\n",
    "gru_final = GRU(1280,512)\n",
    "\n",
    "# W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 18), mean=0, stddev=0.01),trainable=True)\n",
    "# b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(18,), mean=0, stddev=0.01),trainable=True)\n",
    "\n",
    "W_output = weights['out']\n",
    "b_output = biases['out']\n",
    "\n",
    "# output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "output1 = tf.nn.relu(tf.matmul(gru_final.h_t,W_output)+b_output)\n",
    "# out2 = tf.matmul(gru.h_t[0], W_output)+b_output\n",
    "\n",
    "# tf.get_variable('W7', shape=(1024,50), initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "out3 = gru.h_t\n",
    "out4 = gru_final.h_t\n",
    "print(out3.shape)\n",
    "print(out4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "(59,)\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "y_train = my_dateset.X\n",
    "y_train=np.array(y_train)\n",
    "        \n",
    "for i in range(len(y_train)):\n",
    "    y_train[i]=np.array(y_train[i])\n",
    "#     for j in range(len(x1[i])):\n",
    "#         x1[i][j]=np.array(x1[i][j])\n",
    "print(y_train.shape)\n",
    "print(y_train[0].shape)\n",
    "    \n",
    "\n",
    "# x1=tf.constant(x1[0])\n",
    "print(\"hi\")\n",
    "            \n",
    "VOCAB_LEN=19\n",
    "EMBED_SIZE=50\n",
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_LEN, EMBED_SIZE]))\n",
    "caption_p = tf.placeholder(dtype=tf.int32, shape=(None,None), name='caption_p')\n",
    "embed = tf.nn.embedding_lookup(embeddings, caption_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "expected = my_dateset.y\n",
    "expected=np.array(expected)\n",
    "for e in range(len(expected)):\n",
    "    expected[e]=np.array(expected[e])\n",
    "print(expected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = tf.placeholder(dtype=tf.float64, shape=(None,None,None), name='expected_output')\n",
    "loss = tf.reduce_sum(tf.squared_difference(output1 ,expected_output)) #/ float(1)\n",
    "# e_loss = tf.placeholder(dtype=tf.float64,name='e_loss')\n",
    "# epoch_loss = tf.reduce_sum(e_loss)\n",
    "# train2 = tf.train.AdamOptimizer(0.0001).minimize(tf.reduce_sum(e_loss))\n",
    "# train_step = tf.train.AdamOptimizer().minimize()\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "def pad(batch_y):\n",
    "    print(batch_y.shape)\n",
    "    x=0\n",
    "    for y in batch_y:\n",
    "        if(len(y)>x):\n",
    "            x=len(y)\n",
    "#     x = functools.reduce(lambda x,y: len(x) if(len(x)>len(y)) else len(y),batch_y)\n",
    "    \n",
    "    ret = []\n",
    "    for y in range(len(batch_y)):\n",
    "        res=np.zeros(x)\n",
    "        s = batch_y[y]\n",
    "        res[0:len(s)]=batch_y[y]\n",
    "#         batch_y[y]=res\n",
    "        ret.append(res)\n",
    "    return np.array(ret)\n",
    "        \n",
    "        \n",
    "# a=[[1,2],[1,2,3]]\n",
    "# pad(a)\n",
    "\n",
    "def pad2(batch_ex):\n",
    "#     r = functools.reduce(lambda x,y: len(x) if(len(x)>len(y)) else len(y),batch_ex)\n",
    "#     print(\":::::\",r)\n",
    "    r=0\n",
    "    c=0\n",
    "    for ex in batch_ex:\n",
    "        shape = ex.shape\n",
    "#         print(shape)\n",
    "        if(shape[0]>r):\n",
    "            r=shape[0]\n",
    "        if(shape[1]>c):\n",
    "            c=shape[1]\n",
    "#     c = functools.reduce(lambda x,y: len(x[0]) if(len(x[0])>len(y[0])) else len(y[0]),batch_ex)\n",
    "#     print(\":::::\",c)\n",
    "#     print(r,c)\n",
    "    ret=[]\n",
    "    for ex in batch_ex:\n",
    "        res=np.zeros((r,c))\n",
    "#         print(res.shape)\n",
    "#         print(ex.shape)\n",
    "        res[0:ex.shape[0],0:ex.shape[1]]=ex\n",
    "        ret.append(res)\n",
    "#     print(ret)\n",
    "        \n",
    "    return(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "batch  0\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 94, 50)\n",
      "gru1: (10, 94, 256)\n",
      "ktile: (10, 94, 1024)\n",
      "emb: (10, 94, 1280)\n",
      "inp: (10, 94, 1280)\n",
      "a: (10, 94, 18) [[[0.26873727 0.         0.06337995 ... 0.         0.         0.        ]\n",
      "  [0.34559651 0.         0.1407323  ... 0.         0.         0.        ]\n",
      "  [0.37988009 0.         0.17982864 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.42133546 0.0291156  0.19982977 ... 0.         0.         0.        ]\n",
      "  [0.42133546 0.0291156  0.19982977 ... 0.         0.         0.        ]\n",
      "  [0.42133546 0.0291156  0.19982977 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.16275945 0.         0.05623342 ... 0.         0.         0.07293346]\n",
      "  [0.16528865 0.         0.10815651 ... 0.         0.         0.20607165]\n",
      "  [0.15248696 0.         0.12788167 ... 0.         0.         0.27654055]\n",
      "  ...\n",
      "  [0.12134358 0.         0.1404042  ... 0.         0.         0.35440135]\n",
      "  [0.12134216 0.         0.13899034 ... 0.         0.         0.35590574]\n",
      "  [0.12110598 0.         0.13791154 ... 0.         0.         0.35730878]]\n",
      "\n",
      " [[0.01086623 0.         0.         ... 0.         0.         0.07093815]\n",
      "  [0.         0.         0.         ... 0.         0.01217879 0.20375948]\n",
      "  [0.         0.         0.         ... 0.         0.06547943 0.27262735]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.12779565 0.34657482]\n",
      "  [0.         0.         0.         ... 0.         0.12779583 0.34657562]\n",
      "  [0.         0.         0.         ... 0.         0.12779597 0.34657612]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.10385788 0.         ... 0.         0.         0.01173134]\n",
      "  [0.         0.17908191 0.         ... 0.         0.         0.10716507]\n",
      "  [0.         0.19278215 0.         ... 0.         0.         0.15329174]\n",
      "  ...\n",
      "  [0.         0.15589907 0.         ... 0.         0.         0.19874993]\n",
      "  [0.         0.15589898 0.         ... 0.         0.         0.19875038]\n",
      "  [0.         0.15589891 0.         ... 0.         0.         0.19875068]]\n",
      "\n",
      " [[0.         0.22938855 0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.40974972 0.04242014 ... 0.         0.         0.        ]\n",
      "  [0.         0.49725736 0.07312265 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.5802068  0.11034677 ... 0.         0.         0.        ]\n",
      "  [0.         0.5802068  0.11034679 ... 0.         0.         0.        ]\n",
      "  [0.         0.5802068  0.1103468  ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.06417212 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.12792835 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.15739064 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.17533697 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.17533684 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.17533679 ... 0.         0.         0.        ]]] \n",
      "\n",
      "175.7169105161877\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 78, 50)\n",
      "gru1: (10, 78, 256)\n",
      "ktile: (10, 78, 1024)\n",
      "emb: (10, 78, 1280)\n",
      "inp: (10, 78, 1280)\n",
      "a: (10, 78, 18) [[[0.         0.         0.         ... 0.35979486 0.         0.27255841]\n",
      "  [0.         0.         0.01404561 ... 0.41194807 0.         0.19612675]\n",
      "  [0.         0.         0.03304889 ... 0.44022583 0.         0.15292578]\n",
      "  ...\n",
      "  [0.         0.         0.06620959 ... 0.45874125 0.         0.10228378]\n",
      "  [0.         0.         0.0661413  ... 0.45799292 0.         0.10271958]\n",
      "  [0.         0.         0.06641902 ... 0.45759734 0.         0.10298858]]\n",
      "\n",
      " [[0.10521602 0.         0.         ... 0.18349516 0.         0.66858094]\n",
      "  [0.19845755 0.         0.         ... 0.12361975 0.         0.79978252]\n",
      "  [0.24872204 0.         0.         ... 0.08809475 0.         0.8627755 ]\n",
      "  ...\n",
      "  [0.31403702 0.         0.         ... 0.0380932  0.         0.92441726]\n",
      "  [0.31387339 0.         0.         ... 0.03766882 0.         0.92464591]\n",
      "  [0.31391394 0.         0.         ... 0.03757277 0.         0.9247989 ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.24935718 0.         0.35693569]\n",
      "  [0.         0.         0.         ... 0.25704108 0.         0.33206507]\n",
      "  [0.         0.         0.         ... 0.27062117 0.         0.31661014]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.30607913 0.         0.29652941]\n",
      "  [0.         0.         0.         ... 0.30446796 0.         0.29698794]\n",
      "  [0.         0.         0.         ... 0.30344632 0.         0.29733131]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.07494409 0.         0.10154081 ... 0.         0.         0.53182773]\n",
      "  [0.13293653 0.         0.18204179 ... 0.         0.03626213 0.59327259]\n",
      "  [0.15420604 0.         0.22456066 ... 0.         0.10025613 0.61989275]\n",
      "  ...\n",
      "  [0.163525   0.         0.27996533 ... 0.         0.15640356 0.63702544]\n",
      "  [0.1635247  0.         0.27996546 ... 0.         0.1564033  0.63702515]\n",
      "  [0.16352449 0.         0.27996557 ... 0.         0.15640313 0.63702496]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.31497647]\n",
      "  [0.         0.         0.         ... 0.         0.         0.26678338]\n",
      "  [0.         0.         0.         ... 0.         0.         0.23687757]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.19221099]\n",
      "  [0.         0.         0.         ... 0.         0.         0.19219759]\n",
      "  [0.         0.         0.         ... 0.         0.         0.19210001]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.45901797 0.         0.11217205]\n",
      "  [0.         0.08108781 0.         ... 0.54762442 0.         0.        ]\n",
      "  [0.         0.17595476 0.         ... 0.58891363 0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.29247443 0.         ... 0.61871761 0.         0.        ]\n",
      "  [0.         0.29240772 0.         ... 0.61696155 0.         0.        ]\n",
      "  [0.         0.29249027 0.         ... 0.61580841 0.         0.        ]]] \n",
      "\n",
      "151.30106158628405\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 76, 50)\n",
      "gru1: (10, 76, 256)\n",
      "ktile: (10, 76, 1024)\n",
      "emb: (10, 76, 1280)\n",
      "inp: (10, 76, 1280)\n",
      "a: (10, 76, 18) [[[0.16831164 0.47962275 0.3503197  ... 0.         0.2468662  0.        ]\n",
      "  [0.12776511 0.51985647 0.36129126 ... 0.         0.25473157 0.        ]\n",
      "  [0.10332951 0.53703883 0.36708613 ... 0.         0.25597737 0.        ]\n",
      "  ...\n",
      "  [0.06814166 0.54713539 0.37452088 ... 0.         0.25407256 0.        ]\n",
      "  [0.06743941 0.54800658 0.37406413 ... 0.         0.25396887 0.        ]\n",
      "  [0.06664951 0.54878907 0.37374357 ... 0.         0.25435184 0.        ]]\n",
      "\n",
      " [[0.21982091 0.18300924 0.08455655 ... 0.13089061 0.10204111 0.        ]\n",
      "  [0.17609663 0.07730379 0.         ... 0.19708145 0.05747713 0.        ]\n",
      "  [0.13540727 0.02120362 0.         ... 0.23444083 0.0416517  0.        ]\n",
      "  ...\n",
      "  [0.04925275 0.         0.         ... 0.28592266 0.04209424 0.        ]\n",
      "  [0.04925149 0.         0.         ... 0.28592219 0.04209459 0.        ]\n",
      "  [0.04925071 0.         0.         ... 0.28592183 0.04209476 0.        ]]\n",
      "\n",
      " [[0.19503641 0.17208291 0.4770438  ... 0.08743953 0.29969477 0.        ]\n",
      "  [0.18047421 0.07456032 0.56171622 ... 0.1188838  0.3377803  0.        ]\n",
      "  [0.17748833 0.03017497 0.6083147  ... 0.13090181 0.35391652 0.        ]\n",
      "  ...\n",
      "  [0.188752   0.         0.66064892 ... 0.13226112 0.36708234 0.        ]\n",
      "  [0.18767629 0.         0.66005164 ... 0.13265785 0.36684746 0.        ]\n",
      "  [0.18670109 0.         0.65963603 ... 0.13328146 0.36712337 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.47241464 0.27631582 0.31780061 ... 0.         0.10955321 0.        ]\n",
      "  [0.62437038 0.22467502 0.33259217 ... 0.         0.05698568 0.        ]\n",
      "  [0.7153568  0.19728495 0.34861307 ... 0.         0.02945661 0.0245634 ]\n",
      "  ...\n",
      "  [0.83912985 0.1574283  0.38861582 ... 0.         0.         0.07723751]\n",
      "  [0.8379993  0.15779504 0.3882737  ... 0.         0.         0.07606761]\n",
      "  [0.83693531 0.15818643 0.38801318 ... 0.         0.         0.07497018]]\n",
      "\n",
      " [[0.06553892 0.50606129 0.44075322 ... 0.         0.35985838 0.        ]\n",
      "  [0.         0.55813724 0.50556521 ... 0.         0.44305208 0.        ]\n",
      "  [0.         0.58036376 0.5421516  ... 0.         0.48884931 0.        ]\n",
      "  ...\n",
      "  [0.         0.59800373 0.58883083 ... 0.         0.54204156 0.        ]\n",
      "  [0.         0.59800371 0.58883084 ... 0.         0.54204157 0.        ]\n",
      "  [0.         0.59800371 0.58883084 ... 0.         0.54204157 0.        ]]\n",
      "\n",
      " [[0.26571277 0.43566691 0.28239248 ... 0.06580802 0.26104498 0.        ]\n",
      "  [0.258974   0.45363254 0.26327637 ... 0.08102019 0.30434297 0.        ]\n",
      "  [0.24620017 0.4589168  0.25552019 ... 0.08334379 0.33419701 0.        ]\n",
      "  ...\n",
      "  [0.22217122 0.45955897 0.2532911  ... 0.07426589 0.38242666 0.        ]\n",
      "  [0.2222592  0.45954552 0.25279519 ... 0.07290337 0.38049312 0.        ]\n",
      "  [0.22213951 0.45992614 0.25216757 ... 0.07146948 0.37863057 0.        ]]] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.89822156197027\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 78, 50)\n",
      "gru1: (10, 78, 256)\n",
      "ktile: (10, 78, 1024)\n",
      "emb: (10, 78, 1280)\n",
      "inp: (10, 78, 1280)\n",
      "a: (10, 78, 18) [[[0.         0.19759184 0.41001247 ... 0.         0.         0.07139155]\n",
      "  [0.         0.16495003 0.50865678 ... 0.         0.         0.04723488]\n",
      "  [0.         0.15244509 0.55505177 ... 0.01906249 0.         0.03257982]\n",
      "  ...\n",
      "  [0.         0.14754076 0.60348916 ... 0.0739006  0.         0.00901764]\n",
      "  [0.         0.14812452 0.60263601 ... 0.07244799 0.         0.01007899]\n",
      "  [0.         0.14871711 0.60170779 ... 0.0712065  0.         0.01133951]]\n",
      "\n",
      " [[0.17740001 0.25412173 0.         ... 0.         0.40867698 0.        ]\n",
      "  [0.2229695  0.22591774 0.         ... 0.         0.55661624 0.        ]\n",
      "  [0.24556635 0.20558461 0.         ... 0.         0.63749286 0.        ]\n",
      "  ...\n",
      "  [0.2760407  0.18109743 0.         ... 0.         0.74080008 0.        ]\n",
      "  [0.27514886 0.18094544 0.         ... 0.         0.74000834 0.        ]\n",
      "  [0.27418528 0.18074703 0.         ... 0.         0.73866787 0.        ]]\n",
      "\n",
      " [[0.17951559 0.21346977 0.50640567 ... 0.         0.         0.        ]\n",
      "  [0.23679778 0.17619687 0.65463193 ... 0.         0.         0.        ]\n",
      "  [0.27058528 0.15574441 0.72716038 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.31682947 0.1385297  0.80574646 ... 0.         0.         0.        ]\n",
      "  [0.31682947 0.1385297  0.80574646 ... 0.         0.         0.        ]\n",
      "  [0.31682947 0.1385297  0.80574646 ... 0.         0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.11972051 0.11506044 0.13249148 ... 0.         0.06615298 0.17273545]\n",
      "  [0.14823888 0.03082971 0.09119454 ... 0.         0.02089167 0.20656279]\n",
      "  [0.16774745 0.         0.06556554 ... 0.         0.         0.22385921]\n",
      "  ...\n",
      "  [0.1969877  0.         0.0260653  ... 0.         0.         0.23751564]\n",
      "  [0.19666567 0.         0.02552311 ... 0.         0.         0.23848487]\n",
      "  [0.19662851 0.         0.02469821 ... 0.         0.         0.23973359]]\n",
      "\n",
      " [[0.         0.37242517 0.33398519 ... 0.         0.07912317 0.17027815]\n",
      "  [0.         0.44188808 0.40622075 ... 0.         0.04392056 0.2142613 ]\n",
      "  [0.         0.48605924 0.44524927 ... 0.         0.02596522 0.2397656 ]\n",
      "  ...\n",
      "  [0.         0.54696105 0.4922452  ... 0.         0.00814496 0.26985989]\n",
      "  [0.         0.54696107 0.49224519 ... 0.         0.00814497 0.26985991]\n",
      "  [0.         0.54696109 0.49224518 ... 0.         0.00814498 0.26985992]]\n",
      "\n",
      " [[0.42066794 0.46124768 0.         ... 0.         0.31101649 0.03721107]\n",
      "  [0.57715786 0.56081496 0.         ... 0.         0.35712906 0.        ]\n",
      "  [0.65168628 0.61434655 0.         ... 0.         0.3645994  0.        ]\n",
      "  ...\n",
      "  [0.72237204 0.67544515 0.         ... 0.         0.35294567 0.        ]\n",
      "  [0.7224655  0.67554754 0.         ... 0.         0.35232146 0.        ]\n",
      "  [0.72263876 0.67574863 0.         ... 0.         0.35207397 0.        ]]] \n",
      "\n",
      "144.80277582268536\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 78, 50)\n",
      "gru1: (10, 78, 256)\n",
      "ktile: (10, 78, 1024)\n",
      "emb: (10, 78, 1280)\n",
      "inp: (10, 78, 1280)\n",
      "a: (10, 78, 18) [[[0.         0.         0.06846141 ... 0.41736344 0.23717618 0.26480942]\n",
      "  [0.         0.         0.         ... 0.50216285 0.40295387 0.24575429]\n",
      "  [0.         0.         0.         ... 0.54039272 0.48118695 0.2315552 ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.57357745 0.55344494 0.20460675]\n",
      "  [0.         0.         0.         ... 0.57472198 0.55321947 0.2039909 ]\n",
      "  [0.         0.         0.         ... 0.57561978 0.55326764 0.20373449]]\n",
      "\n",
      " [[0.         0.04893463 0.23096414 ... 0.07803881 0.         0.0481715 ]\n",
      "  [0.         0.08781361 0.22364645 ... 0.         0.         0.        ]\n",
      "  [0.         0.11089164 0.21409372 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.13304908 0.20503602 ... 0.         0.         0.        ]\n",
      "  [0.         0.13068963 0.20514031 ... 0.         0.         0.        ]\n",
      "  [0.         0.12908181 0.20524533 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.03617173 0.         ... 0.31594262 0.         0.68624591]\n",
      "  [0.         0.09011967 0.         ... 0.36382399 0.         0.87461513]\n",
      "  [0.         0.13041084 0.         ... 0.39170054 0.         0.96221264]\n",
      "  ...\n",
      "  [0.         0.18265605 0.         ... 0.44090025 0.         1.04142518]\n",
      "  [0.         0.18147138 0.         ... 0.4412063  0.         1.04148187]\n",
      "  [0.         0.18069921 0.         ... 0.44137965 0.         1.04151578]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.22149468 0.7224579  ... 0.24747412 0.08909887 0.38855316]\n",
      "  [0.         0.32836628 0.99356845 ... 0.27285312 0.17785274 0.45074144]\n",
      "  [0.         0.37674861 1.13747001 ... 0.29380953 0.21654708 0.48882232]\n",
      "  ...\n",
      "  [0.00496808 0.40281883 1.30961978 ... 0.34130681 0.24481405 0.54774059]\n",
      "  [0.00496808 0.40281884 1.30961978 ... 0.34130681 0.24481405 0.54774059]\n",
      "  [0.00496808 0.40281884 1.30961978 ... 0.34130681 0.24481405 0.54774059]]\n",
      "\n",
      " [[0.05954398 0.         0.01579062 ... 0.         0.         0.50056807]\n",
      "  [0.17314693 0.         0.         ... 0.         0.05125354 0.62315058]\n",
      "  [0.23177483 0.         0.         ... 0.         0.07691978 0.68965676]\n",
      "  ...\n",
      "  [0.31161517 0.         0.         ... 0.         0.09502157 0.76869046]\n",
      "  [0.31161518 0.         0.         ... 0.         0.09502157 0.76869046]\n",
      "  [0.31161518 0.         0.         ... 0.         0.09502157 0.76869046]]\n",
      "\n",
      " [[0.         0.         0.30013954 ... 0.17841623 0.         0.16876254]\n",
      "  [0.         0.         0.34747789 ... 0.16112122 0.         0.13832598]\n",
      "  [0.         0.         0.37070041 ... 0.15644787 0.         0.13254199]\n",
      "  ...\n",
      "  [0.         0.         0.39578985 ... 0.16441809 0.         0.13817783]\n",
      "  [0.         0.         0.39578866 ... 0.1645983  0.         0.13831187]\n",
      "  [0.         0.         0.39578334 ... 0.16469349 0.         0.13838873]]] \n",
      "\n",
      "167.16941915264022\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "1\n",
      "batch  0\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 94, 50)\n",
      "gru1: (10, 94, 256)\n",
      "ktile: (10, 94, 1024)\n",
      "emb: (10, 94, 1280)\n",
      "inp: (10, 94, 1280)\n",
      "a: (10, 94, 18) [[[0.         0.         0.35262822 ... 0.         0.         0.10474658]\n",
      "  [0.0135855  0.         0.33784159 ... 0.         0.         0.06167341]\n",
      "  [0.03031381 0.         0.32468281 ... 0.         0.         0.03696712]\n",
      "  ...\n",
      "  [0.04656194 0.         0.29570498 ... 0.         0.         0.00750239]\n",
      "  [0.04656194 0.         0.29570498 ... 0.         0.         0.00750239]\n",
      "  [0.04656194 0.         0.29570498 ... 0.         0.         0.0075024 ]]\n",
      "\n",
      " [[0.26697849 0.         0.40155541 ... 0.         0.         0.06340644]\n",
      "  [0.42294225 0.         0.42845468 ... 0.         0.         0.02520597]\n",
      "  [0.4999665  0.         0.44470679 ... 0.         0.         0.01448615]\n",
      "  ...\n",
      "  [0.57176496 0.         0.46201488 ... 0.         0.         0.02325603]\n",
      "  [0.57242531 0.         0.46281851 ... 0.         0.         0.02250506]\n",
      "  [0.57306694 0.         0.46361209 ... 0.         0.         0.02209576]]\n",
      "\n",
      " [[0.06279007 0.         0.34782106 ... 0.         0.         0.00628562]\n",
      "  [0.11141178 0.         0.34010929 ... 0.         0.         0.        ]\n",
      "  [0.13423033 0.         0.33448252 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.15275778 0.         0.31980264 ... 0.02595229 0.         0.        ]\n",
      "  [0.15275754 0.         0.31980189 ... 0.02595128 0.         0.        ]\n",
      "  [0.1527574  0.         0.31980143 ... 0.02595067 0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.56538055 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.65873273 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.70174866 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.74063221 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.74063224 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.74063226 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.04639316 ... 0.         0.         0.35815527]\n",
      "  [0.         0.         0.         ... 0.         0.09223404 0.45525575]\n",
      "  [0.         0.         0.         ... 0.         0.1655072  0.50552119]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.25061471 0.56284882]\n",
      "  [0.         0.         0.         ... 0.         0.25061471 0.56284883]\n",
      "  [0.         0.         0.         ... 0.         0.2506147  0.56284884]]\n",
      "\n",
      " [[0.         0.         0.27717619 ... 0.43074337 0.         0.42563156]\n",
      "  [0.         0.         0.23008383 ... 0.66040849 0.         0.53605226]\n",
      "  [0.         0.         0.20323703 ... 0.76328309 0.         0.58598703]\n",
      "  ...\n",
      "  [0.         0.         0.16432912 ... 0.84601132 0.         0.63698228]\n",
      "  [0.         0.         0.16432877 ... 0.84601001 0.         0.63698143]\n",
      "  [0.         0.         0.16432855 ... 0.8460092  0.         0.63698089]]] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.89212464941772\n",
      "\n",
      "\n",
      "\n",
      "batch  1\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 78, 50)\n",
      "gru1: (10, 78, 256)\n",
      "ktile: (10, 78, 1024)\n",
      "emb: (10, 78, 1280)\n",
      "inp: (10, 78, 1280)\n",
      "a: (10, 78, 18) [[[0.16573632 0.15536953 0.32275532 ... 0.         0.05367948 0.        ]\n",
      "  [0.04841085 0.07041613 0.42025829 ... 0.00722012 0.         0.        ]\n",
      "  [0.         0.02647284 0.4722974  ... 0.06207689 0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.53098723 ... 0.12635103 0.         0.        ]\n",
      "  [0.         0.         0.5307919  ... 0.12847128 0.         0.        ]\n",
      "  [0.         0.         0.53088198 ... 0.13041384 0.         0.        ]]\n",
      "\n",
      " [[0.6509917  0.04719758 0.22370452 ... 0.         0.29995821 0.37462735]\n",
      "  [0.80356072 0.         0.25559282 ... 0.         0.34018967 0.53364063]\n",
      "  [0.87992509 0.         0.27213293 ... 0.         0.35522911 0.60867529]\n",
      "  ...\n",
      "  [0.96487926 0.         0.30965534 ... 0.         0.36278596 0.68519155]\n",
      "  [0.96612647 0.         0.30944794 ... 0.         0.36148806 0.68519892]\n",
      "  [0.96777044 0.         0.30951567 ... 0.         0.36016831 0.68508161]]\n",
      "\n",
      " [[0.24586726 0.44143602 0.         ... 0.         0.13698923 0.01164683]\n",
      "  [0.19872067 0.48694324 0.         ... 0.         0.09988544 0.00270214]\n",
      "  [0.17520353 0.5031971  0.         ... 0.         0.07560747 0.        ]\n",
      "  ...\n",
      "  [0.15039111 0.51897806 0.         ... 0.01555576 0.03956344 0.        ]\n",
      "  [0.15194197 0.51821737 0.         ... 0.01774281 0.03819142 0.        ]\n",
      "  [0.15365296 0.51753953 0.         ... 0.01973483 0.03682904 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.38355539 0.52497375 0.13625892 ... 0.         0.37062114 0.        ]\n",
      "  [0.40694928 0.61717224 0.10526088 ... 0.         0.44779945 0.        ]\n",
      "  [0.4191402  0.6567641  0.07721502 ... 0.         0.48162498 0.        ]\n",
      "  ...\n",
      "  [0.43099774 0.6917407  0.0266273  ... 0.         0.50722585 0.        ]\n",
      "  [0.43099798 0.6917408  0.02662739 ... 0.         0.50722616 0.        ]\n",
      "  [0.43099809 0.69174087 0.02662748 ... 0.         0.50722638 0.        ]]\n",
      "\n",
      " [[0.25042119 0.20537149 0.40797837 ... 0.         0.07196846 0.08382094]\n",
      "  [0.22188709 0.15806166 0.55622926 ... 0.         0.0101737  0.13445094]\n",
      "  [0.21725618 0.13788189 0.63858433 ... 0.         0.         0.17265585]\n",
      "  ...\n",
      "  [0.23897654 0.12725897 0.7415306  ... 0.         0.         0.24477761]\n",
      "  [0.24032296 0.1268321  0.74157074 ... 0.         0.         0.24445458]\n",
      "  [0.24130054 0.12652135 0.74158027 ... 0.         0.         0.24429387]]\n",
      "\n",
      " [[0.4861591  0.27572535 0.01993821 ... 0.         0.         0.43981544]\n",
      "  [0.55370467 0.25669903 0.         ... 0.         0.         0.6412318 ]\n",
      "  [0.58739982 0.24610459 0.         ... 0.         0.         0.74153293]\n",
      "  ...\n",
      "  [0.6275447  0.23038748 0.         ... 0.         0.         0.84879565]\n",
      "  [0.6266397  0.23088535 0.         ... 0.         0.         0.84861003]\n",
      "  [0.62578012 0.2316316  0.         ... 0.         0.         0.84873477]]] \n",
      "\n",
      "162.14469578481814\n",
      "\n",
      "\n",
      "\n",
      "batch  2\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 76, 50)\n",
      "gru1: (10, 76, 256)\n",
      "ktile: (10, 76, 1024)\n",
      "emb: (10, 76, 1280)\n",
      "inp: (10, 76, 1280)\n",
      "a: (10, 76, 18) [[[2.61948965e-01 0.00000000e+00 2.83171127e-01 ... 1.05421644e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [2.60526443e-01 0.00000000e+00 2.52375719e-01 ... 1.89093787e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [2.75212469e-01 0.00000000e+00 2.35591117e-01 ... 2.26125091e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  ...\n",
      "  [3.27032185e-01 0.00000000e+00 2.11584754e-01 ... 2.62390834e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [3.27342520e-01 0.00000000e+00 2.09864249e-01 ... 2.61357457e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [3.27543300e-01 0.00000000e+00 2.08716118e-01 ... 2.60404718e-01\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[3.97940174e-01 0.00000000e+00 2.34184156e-01 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [4.70037698e-01 0.00000000e+00 1.72143545e-01 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [5.21414979e-01 0.00000000e+00 1.38197898e-01 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  ...\n",
      "  [6.05000027e-01 0.00000000e+00 9.16212182e-02 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [6.05000191e-01 0.00000000e+00 9.16207559e-02 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [6.05000276e-01 0.00000000e+00 9.16203988e-02 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[2.17279242e-01 0.00000000e+00 5.50304259e-01 ... 2.73524727e-02\n",
      "   5.60111893e-02 0.00000000e+00]\n",
      "  [1.70346374e-01 0.00000000e+00 6.62755478e-01 ... 7.30052031e-02\n",
      "   6.23746262e-02 0.00000000e+00]\n",
      "  [1.48640099e-01 0.00000000e+00 7.23820423e-01 ... 9.36277242e-02\n",
      "   6.39742822e-02 0.00000000e+00]\n",
      "  ...\n",
      "  [1.29977932e-01 0.00000000e+00 7.97199320e-01 ... 1.24868156e-01\n",
      "   5.93317405e-02 0.00000000e+00]\n",
      "  [1.29827686e-01 0.00000000e+00 7.95374309e-01 ... 1.23926242e-01\n",
      "   6.10568976e-02 0.00000000e+00]\n",
      "  [1.29715662e-01 0.00000000e+00 7.94165526e-01 ... 1.23030349e-01\n",
      "   6.29193251e-02 0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[5.78235781e-01 0.00000000e+00 2.58470329e-01 ... 0.00000000e+00\n",
      "   1.30040386e-01 0.00000000e+00]\n",
      "  [6.95934250e-01 0.00000000e+00 1.83868627e-01 ... 0.00000000e+00\n",
      "   1.71552681e-01 0.00000000e+00]\n",
      "  [7.49240814e-01 0.00000000e+00 1.30960036e-01 ... 0.00000000e+00\n",
      "   1.90161625e-01 0.00000000e+00]\n",
      "  ...\n",
      "  [7.91664791e-01 0.00000000e+00 4.67157866e-02 ... 0.00000000e+00\n",
      "   2.01029031e-01 0.00000000e+00]\n",
      "  [7.91871197e-01 0.00000000e+00 4.53712457e-02 ... 0.00000000e+00\n",
      "   2.02686893e-01 0.00000000e+00]\n",
      "  [7.91926915e-01 0.00000000e+00 4.44535258e-02 ... 0.00000000e+00\n",
      "   2.04575543e-01 0.00000000e+00]]\n",
      "\n",
      " [[3.06337377e-01 0.00000000e+00 4.85393352e-01 ... 1.71876796e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [2.84629974e-01 0.00000000e+00 5.53567904e-01 ... 2.83966150e-01\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [2.65965735e-01 0.00000000e+00 5.87932051e-01 ... 3.34360121e-01\n",
      "   0.00000000e+00 3.21678520e-04]\n",
      "  ...\n",
      "  [2.30670639e-01 1.27954347e-02 6.24503908e-01 ... 3.84205562e-01\n",
      "   0.00000000e+00 6.78389505e-02]\n",
      "  [2.30670671e-01 1.27954549e-02 6.24503892e-01 ... 3.84205571e-01\n",
      "   0.00000000e+00 6.78389328e-02]\n",
      "  [2.30670693e-01 1.27954693e-02 6.24503882e-01 ... 3.84205578e-01\n",
      "   0.00000000e+00 6.78389208e-02]]\n",
      "\n",
      " [[1.10641916e-01 0.00000000e+00 2.39679859e-01 ... 0.00000000e+00\n",
      "   2.22783406e-01 0.00000000e+00]\n",
      "  [6.33528978e-03 0.00000000e+00 1.98634154e-01 ... 0.00000000e+00\n",
      "   3.21519658e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 1.84106135e-01 ... 0.00000000e+00\n",
      "   3.73694171e-01 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 1.89707542e-01 ... 0.00000000e+00\n",
      "   4.31337244e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 1.88306297e-01 ... 0.00000000e+00\n",
      "   4.30798139e-01 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 1.86681089e-01 ... 0.00000000e+00\n",
      "   4.30191476e-01 0.00000000e+00]]] \n",
      "\n",
      "149.55044665969442\n",
      "\n",
      "\n",
      "\n",
      "batch  3\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 78, 50)\n",
      "gru1: (10, 78, 256)\n",
      "ktile: (10, 78, 1024)\n",
      "emb: (10, 78, 1280)\n",
      "inp: (10, 78, 1280)\n",
      "a: (10, 78, 18) [[[0.30730757 0.30269381 0.38853881 ... 0.         0.32858565 0.        ]\n",
      "  [0.29636377 0.31139812 0.40213735 ... 0.         0.29941271 0.        ]\n",
      "  [0.28949286 0.31712905 0.41141045 ... 0.         0.28671776 0.        ]\n",
      "  ...\n",
      "  [0.2783215  0.32621982 0.4197557  ... 0.         0.28369574 0.        ]\n",
      "  [0.28075292 0.32728413 0.41990674 ... 0.         0.28193877 0.        ]\n",
      "  [0.28285761 0.32833022 0.42020994 ... 0.         0.28073518 0.        ]]\n",
      "\n",
      " [[0.52016536 0.11890039 0.47427649 ... 0.         0.50063158 0.06433777]\n",
      "  [0.61584653 0.01965821 0.51684161 ... 0.         0.54655249 0.22145629]\n",
      "  [0.66333715 0.         0.53420051 ... 0.         0.56529437 0.29505111]\n",
      "  ...\n",
      "  [0.7140689  0.         0.5371324  ... 0.         0.58426512 0.3649239 ]\n",
      "  [0.7148925  0.         0.5359707  ... 0.         0.58274222 0.3633198 ]\n",
      "  [0.71563956 0.         0.53474355 ... 0.         0.58132287 0.36173268]]\n",
      "\n",
      " [[0.1344213  0.24166248 0.38899064 ... 0.         0.60325405 0.        ]\n",
      "  [0.05531475 0.21817462 0.40382902 ... 0.         0.71558861 0.        ]\n",
      "  [0.02316702 0.20767783 0.41530467 ... 0.04858218 0.77327542 0.        ]\n",
      "  ...\n",
      "  [0.0077148  0.20559818 0.4333225  ... 0.13621463 0.82989831 0.        ]\n",
      "  [0.0077148  0.20559818 0.4333225  ... 0.13621463 0.82989831 0.        ]\n",
      "  [0.0077148  0.20559818 0.4333225  ... 0.13621463 0.82989831 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.27585761 0.36180703 0.55555636 ... 0.         0.29440632 0.        ]\n",
      "  [0.24965921 0.40591604 0.64221825 ... 0.         0.24807448 0.        ]\n",
      "  [0.23659503 0.43253267 0.68641402 ... 0.         0.22443315 0.        ]\n",
      "  ...\n",
      "  [0.22180235 0.46925878 0.74082135 ... 0.         0.19644287 0.        ]\n",
      "  [0.22433889 0.47053412 0.74073936 ... 0.         0.19472955 0.        ]\n",
      "  [0.2266288  0.47171568 0.74087684 ... 0.         0.19346024 0.        ]]\n",
      "\n",
      " [[0.21341739 0.22807442 0.50564877 ... 0.         0.28417247 0.        ]\n",
      "  [0.13927455 0.20015401 0.57203453 ... 0.         0.20513755 0.        ]\n",
      "  [0.09405978 0.19115833 0.60409156 ... 0.         0.15510146 0.        ]\n",
      "  ...\n",
      "  [0.04179849 0.20275128 0.62723299 ... 0.         0.09025028 0.        ]\n",
      "  [0.04179855 0.20275127 0.62723298 ... 0.         0.09025025 0.        ]\n",
      "  [0.04179859 0.20275126 0.62723298 ... 0.         0.09025022 0.        ]]\n",
      "\n",
      " [[0.27813465 0.19595737 0.51957603 ... 0.         0.42113503 0.        ]\n",
      "  [0.25974795 0.14547701 0.59005205 ... 0.         0.42395785 0.        ]\n",
      "  [0.25126652 0.12026684 0.6258108  ... 0.         0.41881414 0.        ]\n",
      "  ...\n",
      "  [0.24029055 0.10077921 0.66281369 ... 0.         0.40150636 0.        ]\n",
      "  [0.24203656 0.10167905 0.66296983 ... 0.         0.40010694 0.        ]\n",
      "  [0.24364719 0.10262174 0.66321032 ... 0.         0.39914435 0.        ]]] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161.09460503454673\n",
      "\n",
      "\n",
      "\n",
      "batch  4\n",
      "x: 10\n",
      "y: 10\n",
      "ex: 10\n",
      "bex: (10,)\n",
      "(10,)\n",
      "inp2: (10, 78, 50)\n",
      "gru1: (10, 78, 256)\n",
      "ktile: (10, 78, 1024)\n",
      "emb: (10, 78, 1280)\n",
      "inp: (10, 78, 1280)\n",
      "a: (10, 78, 18) [[[0.         0.30350862 0.         ... 0.26818586 0.         0.2961832 ]\n",
      "  [0.         0.27879229 0.         ... 0.31402558 0.         0.34008537]\n",
      "  [0.         0.26967777 0.         ... 0.34814485 0.         0.36413961]\n",
      "  ...\n",
      "  [0.         0.26672535 0.         ... 0.39839634 0.         0.38651603]\n",
      "  [0.         0.26607105 0.         ... 0.40011533 0.         0.38781974]\n",
      "  [0.         0.26544095 0.         ... 0.4018239  0.         0.38895325]]\n",
      "\n",
      " [[0.         0.56754626 0.         ... 0.21873702 0.         0.36335553]\n",
      "  [0.         0.65697913 0.         ... 0.19960855 0.         0.43456841]\n",
      "  [0.         0.69775334 0.         ... 0.18269669 0.         0.47040561]\n",
      "  ...\n",
      "  [0.         0.73080025 0.         ... 0.15123317 0.         0.50376657]\n",
      "  [0.         0.73067974 0.         ... 0.15075712 0.         0.50309203]\n",
      "  [0.         0.73057363 0.         ... 0.15042418 0.         0.50262322]]\n",
      "\n",
      " [[0.         0.24201685 0.         ... 0.20455996 0.08238676 0.13000487]\n",
      "  [0.03696635 0.16676529 0.         ... 0.2136455  0.19407493 0.09123709]\n",
      "  [0.05635916 0.12142897 0.         ... 0.2283853  0.23693673 0.07300895]\n",
      "  ...\n",
      "  [0.06909027 0.04273342 0.         ... 0.2627068  0.25807117 0.03943694]\n",
      "  [0.06915532 0.04267047 0.         ... 0.26248587 0.25804669 0.03907507]\n",
      "  [0.06922062 0.04263163 0.         ... 0.26235072 0.25802415 0.03883246]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.08832701 0.04049435 ... 0.25634352 0.         0.14429064]\n",
      "  [0.         0.         0.08527015 ... 0.26448187 0.         0.12419338]\n",
      "  [0.         0.         0.11312352 ... 0.26518728 0.         0.11972028]\n",
      "  ...\n",
      "  [0.         0.         0.15377989 ... 0.25956618 0.         0.11153744]\n",
      "  [0.         0.         0.15377989 ... 0.25956618 0.         0.11153744]\n",
      "  [0.         0.         0.15377989 ... 0.25956618 0.         0.11153744]]\n",
      "\n",
      " [[0.         0.46914399 0.16739273 ... 0.28923492 0.         0.43299837]\n",
      "  [0.         0.52374154 0.25717331 ... 0.32040455 0.         0.54736802]\n",
      "  [0.         0.55423065 0.2974222  ... 0.33707874 0.         0.60947993]\n",
      "  ...\n",
      "  [0.         0.58622032 0.32484274 ... 0.3572055  0.         0.68488407]\n",
      "  [0.         0.58622032 0.32484274 ... 0.3572055  0.         0.68488407]\n",
      "  [0.         0.58622032 0.32484274 ... 0.3572055  0.         0.68488407]]\n",
      "\n",
      " [[0.         0.28276444 0.16184249 ... 0.23809245 0.         0.09477995]\n",
      "  [0.         0.25825447 0.25946406 ... 0.2358178  0.         0.02276655]\n",
      "  [0.         0.25380636 0.31166746 ... 0.23000522 0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.26078831 0.37309887 ... 0.20982644 0.         0.        ]\n",
      "  [0.         0.26076318 0.37297196 ... 0.20964183 0.         0.        ]\n",
      "  [0.         0.26074086 0.37290804 ... 0.20952428 0.         0.        ]]] \n",
      "\n",
      "149.3615654982895\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch = 2\n",
    "vocab_size = 19\n",
    "batch_size=10\n",
    "\n",
    "x_train = my_dateset.images\n",
    "caption = my_dateset.X\n",
    "expected = my_dateset.y\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    loss_ar=[]\n",
    "    for e in range(epoch):\n",
    "        loss_no=[]\n",
    "        print(e)\n",
    "        for batch in range(len(x_train)//batch_size):\n",
    "            print(\"batch \",batch)\n",
    "            batch_x = x_train[batch*batch_size:min((batch+1)*batch_size,len(x_train))]\n",
    "            batch_y = caption[batch*batch_size:min((batch+1)*batch_size,len(caption))] \n",
    "            batch_ex = expected[batch*batch_size:min((batch+1)*batch_size,len(expected))]\n",
    "            print(\"x:\",len(batch_x))\n",
    "            print(\"y:\",len(batch_y))\n",
    "            print(\"ex:\",len(batch_ex))\n",
    "            \n",
    "#             print(batch_y)\n",
    "            \n",
    "            batch_x = np.array(batch_x)\n",
    "            for b in range(len(batch_x)):\n",
    "                batch_x[b]=np.array(batch_x[b])\n",
    "            batch_y = np.array(batch_y)\n",
    "            for b in range(len(batch_y)):\n",
    "                batch_y[b]=np.array(batch_y[b])\n",
    "            batch_ex = np.array(batch_ex)\n",
    "            for b in range(len(batch_ex)):\n",
    "                batch_ex[b]=np.array(batch_ex[b])\n",
    "                \n",
    "            print(\"bex:\",batch_ex.shape)\n",
    "#             print(batch_ex[0].shape)\n",
    "                \n",
    "            batch_y = pad(batch_y)\n",
    "            batch_ex = pad2(batch_ex)\n",
    "            \n",
    "#             batch_y = batch_y.reshape((-1,1))\n",
    "#             print(batch_ex.shape)\n",
    "            \n",
    "            sess.run(init)\n",
    "            c,m = sess.run([output_train,model_train],feed_dict={im:batch_x})\n",
    "            \n",
    "            inp2 = sess.run(embed,feed_dict={caption_p:batch_y})\n",
    "            print(\"inp2:\",inp2.shape)\n",
    "            gru1 = sess.run(out3,feed_dict={gru.input_layer:inp2})\n",
    "            print(\"gru1:\", gru1.shape)\n",
    "            features_try = K.tile(K.expand_dims(c, 1), [1, K.shape(inp2)[1], 1])\n",
    "            print(\"ktile:\",features_try.shape)\n",
    "            embeddings = tf.concat([features_try,gru1],2)\n",
    "            print(\"emb:\",embeddings.shape)\n",
    "            inp = sess.run(embeddings)\n",
    "            print(\"inp:\",inp.shape)\n",
    "            a = sess.run(output1,feed_dict={gru_final.input_layer:inp})\n",
    "            print(\"a:\",a.shape,a,\"\\n\")\n",
    "\n",
    "            # print(\"Sssss:\",ex.shape)\n",
    "            ls,tr = sess.run([loss,train_step],feed_dict ={expected_output:batch_ex,gru_final.input_layer:inp})\n",
    "            print(ls/batch_size)\n",
    "            loss_no.append(ls/batch_size)\n",
    "            print(\"\\n\\n\")\n",
    "#         el = sess.run(epoch_loss,feed_dict={e_loss:loss_no})\n",
    "        loss_ar.append(loss_no)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------\") \n",
    "    save_path = saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[175.7169105161877, 151.30106158628405, 151.89822156197027, 144.80277582268536, 167.16941915264022], [188.89212464941772, 162.14469578481814, 149.55044665969442, 161.09460503454673, 149.3615654982895]]\n"
     ]
    }
   ],
   "source": [
    "print(loss_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def load_val_images(data_dir):\n",
    "    image_filenames =[]\n",
    "    images = []\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if filename[-3:] == \"png\":\n",
    "            image_filenames.append(filename)\n",
    "    for name in image_filenames:\n",
    "        image = resize_img(data_dir+name)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_words = []\n",
    "star_text = '<START> '\n",
    "image = load_val_images('all_data/')[0]\n",
    "img_tensor=np.expand_dims(np.array(image),0)\n",
    "img_tensor=np.array(img_tensor)\n",
    "''''image = Variable(torch.FloatTensor([image]))'''\n",
    "predicted = '<START> '\n",
    " \n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 1, 1074) for Tensor 'input_2:0', which has shape '(?, ?, 50)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d0d3d7de4a46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures_try\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minp2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1149\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1150\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 1, 1074) for Tensor 'input_2:0', which has shape '(?, ?, 50)'"
     ]
    }
   ],
   "source": [
    "predicted='<START>'\n",
    "with tf.Session() as sess:\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "#     print(\"####\",weights['W_conv1'].eval())\n",
    "    for di in range(50):\n",
    "        sequence = my_dateset.tokenizer.texts_to_sequences([star_text])[0]\n",
    "#         print(sequence)\n",
    "        decoder_input = np.array(sequence).reshape(-1,1)\n",
    "#         c = sess.run(model1,feed_dict={im:img_tensor})\n",
    "#         print(\"--------------------------\")\n",
    "        c,m = sess.run([output_test,model_test],feed_dict={im:img_tensor})\n",
    "#         print(di, c)\n",
    "#         print(\"--------------------------\")\n",
    "        \n",
    "        inp2 = sess.run(embed,feed_dict={caption_p:decoder_input})\n",
    "        features_try = K.tile(K.expand_dims(c, 1), [1, K.shape(inp2)[1], 1])\n",
    "        embeddings = tf.concat([features_try,inp2],2)\n",
    "        inp = sess.run(embeddings)\n",
    "        a = sess.run(output1,feed_dict={gru.input_layer:inp})\n",
    "        data=list(a[0][0])\n",
    "        i=data.index(max(data))\n",
    "        word = word_for_id(i,my_dateset.tokenizer)\n",
    "        if word is None:\n",
    "            continue\n",
    "        predicted += word + ' '\n",
    "        star_text = word\n",
    "        print(predicted)\n",
    "        if word == '<END>':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
