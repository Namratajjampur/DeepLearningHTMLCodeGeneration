{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'W_conv1': tf.get_variable('W0', shape=(3,3,3,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv2': tf.get_variable('W1', shape=(3,3,32,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv3': tf.get_variable('W2', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv4': tf.get_variable('W3', shape=(3,3,64,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv5': tf.get_variable('W4', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_conv6': tf.get_variable('W5', shape=(3,3,128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_fc1': tf.get_variable('W6', shape=(28*28*128,1024), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'W_fc2': tf.get_variable('W7', shape=(1024,50), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "#     'out': tf.get_variable('W8', shape=(1024,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    }\n",
    "biases = {\n",
    "    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc2': tf.get_variable('B1', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc3': tf.get_variable('B2', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc4': tf.get_variable('B3', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc5': tf.get_variable('B4', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc6': tf.get_variable('B5', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'b_fc1': tf.get_variable('B6', shape=(1024), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'b_fc2': tf.get_variable('B7', shape=(50), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'out': tf.get_variable('B8', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "    \n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_layer=[]\n",
    "        \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "        \n",
    "        print(\"h_t:\",h_t.shape)\n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "def cnn(x,weights,biases):\n",
    "    print(\"in cnn\")\n",
    "    '''\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,1,32])),#56\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,32,32])),#56\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,32,64])),#28\n",
    "               'W_conv4':tf.Variable(tf.random_normal([3,3,64,64])),#28\n",
    "               'W_conv5':tf.Variable(tf.random_normal([3,3,64,128])),#14\n",
    "               'W_conv6':tf.Variable(tf.random_normal([3,3,128,128])),#14\n",
    "               'W_fc1':tf.Variable(tf.random_normal([7*7*128,1024])),  # since 3 times maxpooling.. inputsize/2^3\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1024,1024]))\n",
    "              }\n",
    "                  # depending on what that repeat vector does\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv3':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv4':tf.Variable(tf.random_normal([64])),\n",
    "               'b_conv5':tf.Variable(tf.random_normal([128])),\n",
    "               'b_conv6':tf.Variable(tf.random_normal([128])),\n",
    "               'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
    "               'b_fc2':tf.Variable(tf.random_normal([1024]))\n",
    "             }\n",
    "    '''\n",
    "    \n",
    "    print(\"-1\")\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    print(\"00\")\n",
    "    print(\"bef\",x.shape)\n",
    "    x = tf.reshape(x, shape=[1, 224, 224, 3])\n",
    "    print(\"aft\",x.shape)\n",
    "    print(\"0\")\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'])+  biases['bc1'])\n",
    "    print(\"1\")\n",
    "    print(\"conv1:\",conv1.shape)\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['bc2'])\n",
    "    print(\"2\")\n",
    "    print(\"conv2:\",conv2.shape)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(\"3\")\n",
    "    print(\"maxpool:\",conv2.shape)\n",
    "    conv2 = tf.nn.dropout(conv2, 0.25)\n",
    "#     print(\"dropout:\",conv2.shape)\n",
    "    print(\"okay\")\n",
    "    \n",
    "    conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3']) + biases['bc3'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4']) + biases['bc4'])\n",
    "    print(\"conv3:\",conv3.shape)\n",
    "    #conv4 = conv3\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(\"maxpool:\",conv4.shape)\n",
    "    conv4 = tf.nn.dropout(conv4, 0.25)\n",
    "    \n",
    "    conv5 = tf.nn.relu(conv2d(conv4, weights['W_conv5']) + biases['bc5'])\n",
    "    print(\"conv5:\",conv5.shape)\n",
    "    conv6 = tf.nn.relu(conv2d(conv5, weights['W_conv6']) + biases['bc6'])\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    #conv6 = conv5\n",
    "    conv6 = maxpool2d(conv6)\n",
    "    print(\"conv6:\",conv6.shape)\n",
    "    conv6 = tf.nn.dropout(conv6, 0.25)\n",
    "\n",
    "    fc1 = tf.reshape(conv6,[-1, weights['W_fc1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    print(\"fc1:\",fc1.shape)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.3)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    fc2 = tf.nn.dropout(fc2, 0.3)  \n",
    "    #fc2 = fc1\n",
    "    \n",
    "#     out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    print(\"fc2:\",fc2.shape)\n",
    "    print(fc2)\n",
    "    \n",
    "    x_norm = tf.layers.batch_normalization(fc2, training=True)\n",
    "#     input_gru = tf.repeat(fc2,)\n",
    "    \n",
    "    print(x_norm.shape)\n",
    "    return x_norm\n",
    "\n",
    "# def Gru(hidden_size):  \n",
    "#     gru = GRU(1024,hidden_size)\n",
    "\n",
    "#     W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "#     b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "#     output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "import cv2\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def resize_img(png_file_path):\n",
    "        img_rgb = cv2.imread(png_file_path)\n",
    "        img_grey = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "        img_adapted = cv2.adaptiveThreshold(img_grey, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY, 101, 9)\n",
    "        img_stacked = np.repeat(img_adapted[...,None],3,axis=2)\n",
    "        resized = cv2.resize(img_stacked, (224,224), interpolation=cv2.INTER_AREA)\n",
    "        bg_img = 255 * np.ones(shape=(224,224,3))\n",
    "#         print(bg_img.shape,resized.shape)\n",
    "        bg_img[0:224, 0:224,:] = resized\n",
    "        bg_img /= 255\n",
    "        bg_img = np.rollaxis(bg_img, 2, 0)  \n",
    "#         print(bg_img.shape)\n",
    "        return bg_img\n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, data_dir, input_transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_filenames = []\n",
    "        self.texts = []\n",
    "        all_filenames = listdir(data_dir)\n",
    "        all_filenames.sort()\n",
    "        for filename in (all_filenames):\n",
    "            if filename[-3:] == \"png\":\n",
    "                self.image_filenames.append(filename)\n",
    "            else:\n",
    "                text = '<START> ' + load_doc(self.data_dir+filename) + ' <END>'\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.replace(',', ' ,')\n",
    "                self.texts.append(text)\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Initialize the function to create the vocabulary \n",
    "        tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "        # Create the vocabulary \n",
    "        tokenizer.fit_on_texts([load_doc('vocabulary.vocab')])\n",
    "        self.tokenizer = tokenizer\n",
    "        # Add one spot for the empty word in the vocabulary \n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        # Map the input sentences into the vocabulary indexes\n",
    "        self.train_sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "        # The longest set of boostrap tokens\n",
    "        self.max_sequence = max(len(s) for s in self.train_sequences)\n",
    "        # Specify how many tokens to have in each input sentence\n",
    "        self.max_length = 48\n",
    "        \n",
    "        X, y, image_data_filenames = list(), list(), list()\n",
    "        for img_no, seq in enumerate(self.train_sequences):\n",
    "            in_seq, out_seq = seq[:-1], seq[1:]\n",
    "            out_seq = to_categorical(out_seq, num_classes=self.vocab_size)\n",
    "            image_data_filenames.append(self.image_filenames[img_no])\n",
    "            X.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "                \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.image_data_filenames = image_data_filenames\n",
    "        self.images = list()\n",
    "        for image_name in self.image_data_filenames:\n",
    "            image = resize_img(self.data_dir+image_name)\n",
    "            self.images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# with open(\"dataset.pickle\", \"rb\") as f :\n",
    "#     c = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'all_data/'\n",
    "batch_size = 32\n",
    "my_dateset = Dataset(dir_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(my_dateset.images,dtype=np.float32)\n",
    "for i in range(len(x)):\n",
    "    x[i]=np.array(x[i],dtype=np.float32)\n",
    "# print(x[0].shape)\n",
    "# batch_size = 128\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x))\n",
    "# iterator = dataset.repeat().batch(batch_size).make_initializable_iterator()\n",
    "# data_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cnn\n",
      "-1\n",
      "00\n",
      "bef (1, 3, 224, 224)\n",
      "aft (1, 224, 224, 3)\n",
      "0\n",
      "1\n",
      "conv1: (1, 224, 224, 32)\n",
      "2\n",
      "conv2: (1, 224, 224, 32)\n",
      "3\n",
      "maxpool: (1, 112, 112, 32)\n",
      "WARNING:tensorflow:From <ipython-input-5-45aa0502434e>:44: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "okay\n",
      "conv3: (1, 112, 112, 64)\n",
      "conv3: (1, 112, 112, 64)\n",
      "maxpool: (1, 56, 56, 64)\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "conv5: (1, 56, 56, 128)\n",
      "conv6: (1, 56, 56, 128)\n",
      "conv6: (1, 28, 28, 128)\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "fc1: (1, 1024)\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "fc2: (1, 50)\n",
      "Tensor(\"dropout_4/mul_1:0\", shape=(1, 50), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-45aa0502434e>:79: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "model1 = cnn(x,weights,biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_t: (?, 5)\n",
      "(?, ?, 5)\n"
     ]
    }
   ],
   "source": [
    "gru = GRU(100,5)\n",
    "hidden_size=5\n",
    "W_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "b_output = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "out2 = tf.matmul(gru.h_t[0], W_output)+b_output\n",
    "out3 = gru.h_t\n",
    "print(out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru2 = GRU(1,1)\n",
    "# hidden_size=1\n",
    "# W_output2 = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "# b_output2 = tf.Variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "# output2 = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru2.h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "x1 = my_dateset.X\n",
    "x1=np.array(x1)\n",
    "        \n",
    "for i in range(len(x1)):\n",
    "    x1[i]=np.array(x1[i])\n",
    "\n",
    "x1=tf.constant(x1[0])\n",
    "print(\"hi\")\n",
    "            \n",
    "VOCAB_LEN=19+1\n",
    "EMBED_SIZE=50\n",
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_LEN, EMBED_SIZE]))\n",
    "embed = tf.nn.embedding_lookup(embeddings, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "c: (1, 50)\n",
      "inp2: (76, 50)\n",
      "inp2: (1, 76, 50)\n",
      "exp dims: (1, 1, 50)\n",
      "ktile: (1, 76, 50)\n",
      "emb: (1, 76, 100)\n",
      "a: (1, 76, 5) [[[-2.22663793e-02 -2.42823580e-02 -1.32951676e-02  2.32191393e-02\n",
      "    2.27349301e-02]\n",
      "  [-6.62708723e-03 -3.18418973e-02 -2.20346215e-02  3.61787711e-02\n",
      "    3.00983323e-02]\n",
      "  [-1.03207109e-02 -3.29209287e-02 -3.13912597e-02  2.53027167e-02\n",
      "    3.77070150e-02]\n",
      "  [-1.27648367e-02 -3.38264710e-02 -2.85765055e-02  2.14575047e-02\n",
      "    2.52574068e-02]\n",
      "  [-9.94179323e-03 -2.21592517e-02 -1.83984630e-02  7.13966750e-03\n",
      "    3.08859720e-02]\n",
      "  [-1.97096662e-02 -1.98076713e-02 -9.95720488e-03  1.38238421e-02\n",
      "    1.19553706e-02]\n",
      "  [-1.33873682e-02 -1.53850309e-02 -8.95301198e-03  3.54997352e-03\n",
      "    2.41808582e-02]\n",
      "  [-1.43150967e-02 -2.53299604e-02 -1.72089346e-02  1.12136604e-02\n",
      "    1.84273052e-02]\n",
      "  [-1.07309529e-02 -1.80657228e-02 -1.26030528e-02  2.34120311e-03\n",
      "    2.74259134e-02]\n",
      "  [-1.30175318e-02 -2.66375255e-02 -1.90343516e-02  1.06499735e-02\n",
      "    2.00490528e-02]\n",
      "  [-1.24456493e-02 -1.93450571e-02 -4.02356058e-03  4.02852622e-03\n",
      "    1.16696578e-02]\n",
      "  [-1.01989449e-02 -3.61139674e-02  9.51245045e-04  1.15413189e-02\n",
      "   -1.41344740e-03]\n",
      "  [-1.20896016e-02 -3.52591992e-02 -1.94440033e-02  1.35563571e-02\n",
      "    2.16326158e-02]\n",
      "  [-6.06598862e-03 -2.71820920e-02 -2.05819502e-02  1.54594376e-02\n",
      "    2.76353974e-02]\n",
      "  [-1.00568886e-02 -3.06835091e-02 -3.05840868e-02  1.54383509e-02\n",
      "    3.64016206e-02]\n",
      "  [ 3.67172247e-04 -4.66957640e-02 -3.39489420e-02  2.73215204e-02\n",
      "    2.09848892e-02]\n",
      "  [-3.50800382e-03 -2.84637694e-02 -2.10688301e-02  9.89672298e-03\n",
      "    2.87185585e-02]\n",
      "  [-5.09581379e-03 -4.05478322e-02 -1.20213982e-02  2.47299339e-02\n",
      "    3.69186641e-02]\n",
      "  [-6.19898223e-03 -2.54557216e-02 -9.95085464e-03  8.59052310e-03\n",
      "    3.68046105e-02]\n",
      "  [ 4.50315311e-03 -2.57184604e-02 -1.78720140e-02  1.12949502e-02\n",
      "    2.49402078e-02]\n",
      "  [-3.88385580e-03 -1.88764371e-02 -3.34946022e-03  4.31682680e-03\n",
      "    1.40974877e-02]\n",
      "  [-2.11305034e-03 -1.91953144e-02 -1.22993109e-02  1.10128873e-02\n",
      "    2.37944428e-02]\n",
      "  [-8.17082008e-03 -2.67765249e-02 -2.62801299e-02  1.33055860e-02\n",
      "    3.44133969e-02]\n",
      "  [ 1.26433520e-03 -4.47850830e-02 -3.17479718e-02  2.63188031e-02\n",
      "    1.99819635e-02]\n",
      "  [-3.07714299e-03 -2.75372818e-02 -1.99442737e-02  9.42443581e-03\n",
      "    2.82070091e-02]\n",
      "  [-4.89023868e-03 -4.00919249e-02 -1.14473759e-02  2.45057486e-02\n",
      "    3.66595910e-02]\n",
      "  [-6.10040465e-03 -2.52347930e-02 -9.65786629e-03  8.48477738e-03\n",
      "    3.66727546e-02]\n",
      "  [-8.63201154e-03 -2.52083331e-02 -2.36215177e-02  6.27485507e-03\n",
      "    2.11788277e-02]\n",
      "  [-1.02980183e-02 -1.86342112e-02 -6.33806147e-03  2.00137884e-03\n",
      "    1.22088950e-02]\n",
      "  [-5.24317060e-03 -1.90804222e-02 -1.38502775e-02  9.91860063e-03\n",
      "    2.28556884e-02]\n",
      "  [-9.68523050e-03 -2.67215283e-02 -2.70926260e-02  1.27910104e-02\n",
      "    3.39386964e-02]\n",
      "  [ 5.33558035e-04 -4.47586253e-02 -3.21668791e-02  2.60821165e-02\n",
      "    1.97477088e-02]\n",
      "  [-3.43340017e-03 -2.75246097e-02 -2.01599991e-02  9.31575105e-03\n",
      "    2.80898744e-02]\n",
      "  [-5.06290501e-03 -4.00857093e-02 -1.15583572e-02  2.44555039e-02\n",
      "    3.66013638e-02]\n",
      "  [-6.18455910e-03 -2.52317791e-02 -9.71493028e-03  8.46179648e-03\n",
      "    3.66436098e-02]\n",
      "  [-5.80140795e-03 -3.16639581e-02 -2.42009370e-02  3.36776891e-02\n",
      "    3.70877117e-02]\n",
      "  [-8.89592590e-03 -2.17043897e-02 -6.69903396e-03  1.47461300e-02\n",
      "    2.03435211e-02]\n",
      "  [-4.54872065e-03 -2.05323213e-02 -1.40641174e-02  1.59974232e-02\n",
      "    2.69693941e-02]\n",
      "  [-9.34407868e-03 -2.74102108e-02 -2.72165650e-02  1.56785270e-02\n",
      "    3.60526451e-02]\n",
      "  [ 7.00764880e-04 -4.50849985e-02 -3.22364602e-02  2.74257117e-02\n",
      "    2.08072509e-02]\n",
      "  [-3.35057661e-03 -2.76774869e-02 -2.01985423e-02  9.94072975e-03\n",
      "    2.86271789e-02]\n",
      "  [-5.02210690e-03 -4.01584833e-02 -1.15793689e-02  2.47486049e-02\n",
      "    3.68720529e-02]\n",
      "  [-6.16434352e-03 -2.52657619e-02 -9.72628028e-03  8.59804296e-03\n",
      "    3.67807585e-02]\n",
      "  [-5.79132888e-03 -3.16796037e-02 -2.42070587e-02  3.37413118e-02\n",
      "    3.71572236e-02]\n",
      "  [-8.89092440e-03 -2.17117276e-02 -6.70231061e-03  1.47756720e-02\n",
      "    2.03789399e-02]\n",
      "  [-1.04546856e-02 -1.69250842e-02  2.28519158e-03  5.91437794e-03\n",
      "    1.18428654e-02]\n",
      "  [-9.24743198e-03 -3.49432539e-02  4.16569970e-03  1.24124593e-02\n",
      "   -1.32338484e-03]\n",
      "  [-1.16353814e-02 -3.46854244e-02 -1.77825463e-02  1.39617934e-02\n",
      "    2.16803177e-02]\n",
      "  [ 7.04806545e-03 -3.42280138e-02 -2.44074181e-02  2.66279945e-02\n",
      "    3.90464452e-02]\n",
      "  [-3.68108835e-03 -3.40941764e-02 -3.25295222e-02  2.07454494e-02\n",
      "    4.22183938e-02]\n",
      "  [ 3.45871975e-03 -4.83467562e-02 -3.49221451e-02  2.97911157e-02\n",
      "    2.38775372e-02]\n",
      "  [-1.99331549e-03 -2.92557022e-02 -2.15554778e-02  1.10462403e-02\n",
      "    3.01748795e-02]\n",
      "  [-4.35793365e-03 -4.09336785e-02 -1.22644556e-02  2.52690024e-02\n",
      "    3.76472341e-02]\n",
      "  [-5.83748364e-03 -2.56407145e-02 -1.00723578e-02  8.84127445e-03\n",
      "    3.71713344e-02]\n",
      "  [-8.50361813e-03 -2.54046242e-02 -2.38329138e-02  6.44353445e-03\n",
      "    2.14270607e-02]\n",
      "  [-1.02344917e-02 -1.87296389e-02 -6.44592413e-03  2.08015152e-03\n",
      "    1.23349858e-02]\n",
      "  [-1.11203971e-02 -1.55085652e-02  2.44591417e-03  1.14652553e-05\n",
      "    7.73039769e-03]\n",
      "  [-9.57459290e-03 -3.42808253e-02  4.26116174e-03  9.63577150e-03\n",
      "   -3.38818080e-03]\n",
      "  [-1.17961473e-02 -3.43718885e-02 -1.77271962e-02  1.26432528e-02\n",
      "    2.06195053e-02]\n",
      "  [ 1.13739283e-03 -3.42331499e-02 -1.14713088e-02  2.24113407e-02\n",
      "    1.28679155e-02]\n",
      "  [-6.57993190e-03 -3.42227037e-02 -2.58497501e-02  1.87447397e-02\n",
      "    2.88817524e-02]\n",
      "  [ 2.04125340e-03 -4.84720445e-02 -3.15268254e-02  2.88617392e-02\n",
      "    1.72457815e-02]\n",
      "  [-2.69407590e-03 -2.93504302e-02 -1.98302012e-02  1.06141255e-02\n",
      "    2.68380817e-02]\n",
      "  [-4.70239808e-03 -4.09963791e-02 -1.13883914e-02  2.50666729e-02\n",
      "    3.59787044e-02]\n",
      "  [-6.00778188e-03 -2.56797874e-02 -9.62735250e-03  8.74721255e-03\n",
      "    3.63318235e-02]\n",
      "  [ 4.59668542e-03 -2.58298222e-02 -1.77052918e-02  1.13677948e-02\n",
      "    2.47032136e-02]\n",
      "  [-3.83834784e-03 -1.89325518e-02 -3.26373634e-03  4.35065776e-03\n",
      "    1.39774337e-02]\n",
      "  [ 4.96951229e-03 -2.67968396e-02 -3.94098650e-03  1.84831409e-02\n",
      "    9.46633477e-03]\n",
      "  [-4.75028137e-03 -3.05834920e-02 -2.19374040e-02  1.68599658e-02\n",
      "    2.71188591e-02]\n",
      "  [ 2.91222581e-03 -4.66915541e-02 -2.95261689e-02  2.79753352e-02\n",
      "    1.63556154e-02]\n",
      "  [-2.27544777e-03 -2.84866682e-02 -1.88079440e-02  1.01964774e-02\n",
      "    2.63836464e-02]\n",
      "  [-4.50248131e-03 -4.05711457e-02 -1.08665429e-02  2.48683591e-02\n",
      "    3.57483704e-02]\n",
      "  [-5.91182675e-03 -2.54736217e-02 -9.36096925e-03  8.65363465e-03\n",
      "    3.62145085e-02]\n",
      "  [-8.54138544e-03 -2.53276594e-02 -2.34689456e-02  6.35449245e-03\n",
      "    2.09513263e-02]\n",
      "  [-1.02538891e-02 -1.86941856e-02 -6.25958263e-03  2.03840193e-03\n",
      "    1.20936757e-02]\n",
      "  [-1.11303476e-02 -1.54923439e-02  2.54126956e-03 -8.11951305e-06\n",
      "    7.60799846e-03]]]\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "c: (1, 50)\n",
      "inp2: (76, 50)\n",
      "inp2: (1, 76, 50)\n",
      "exp dims: (1, 1, 50)\n",
      "ktile: (1, 76, 50)\n",
      "emb: (1, 76, 100)\n",
      "a: (1, 76, 5) [[[ 0.00423246 -0.01925197  0.02432759  0.00562012  0.01779606]\n",
      "  [ 0.00235843 -0.03745355  0.05472753  0.00911858  0.0056363 ]\n",
      "  [ 0.01356507 -0.03762328  0.04771571  0.01232016  0.02831627]\n",
      "  [ 0.0272829  -0.0314205   0.04646489  0.00259113  0.02101499]\n",
      "  [ 0.02822642 -0.04450814  0.04217151  0.00712335  0.00717942]\n",
      "  [ 0.0332086  -0.03411761  0.049379    0.02164356  0.01641275]\n",
      "  [ 0.03115859 -0.04584061  0.04364606  0.01656842  0.00487994]\n",
      "  [ 0.03629604 -0.03552903  0.04435707  0.00463685  0.00927345]\n",
      "  [ 0.03275032 -0.04650987  0.04108797  0.00809352  0.00125072]\n",
      "  [ 0.03713187 -0.03585683  0.04304471  0.00035982  0.00744191]\n",
      "  [ 0.02029603 -0.038519    0.04412268  0.01333887  0.00149809]\n",
      "  [ 0.02999002 -0.04523699  0.03864841  0.01467404  0.02791307]\n",
      "  [ 0.02719087 -0.0413841   0.03955634  0.01512538  0.03947456]\n",
      "  [ 0.01987132 -0.04821287  0.05288882  0.0258303   0.03082277]\n",
      "  [ 0.02213513 -0.04278174  0.04674511  0.02083826  0.04099414]\n",
      "  [ 0.03581983 -0.04683313  0.04473183  0.01657579  0.03718981]\n",
      "  [ 0.03241569 -0.05192121  0.0412747   0.01404667  0.0153527 ]\n",
      "  [ 0.02627345 -0.04859899  0.03568001  0.0101652   0.02417659]\n",
      "  [ 0.0277211  -0.05276567  0.03673271  0.0108342   0.00882635]\n",
      "  [ 0.03001822 -0.04322658  0.04408117  0.01529289  0.01065788]\n",
      "  [ 0.01677905 -0.04202442  0.0446799   0.02077197  0.00320359]\n",
      "  [ 0.01481913 -0.04854399  0.05555316  0.02863624  0.01248785]\n",
      "  [ 0.01967952 -0.04295054  0.04810826  0.02224308  0.03178438]\n",
      "  [ 0.03462239 -0.0469187   0.045435    0.01726759  0.03258202]\n",
      "  [ 0.0318299  -0.05196488  0.04163549  0.01438355  0.0130379 ]\n",
      "  [ 0.02599438 -0.04862136  0.035863    0.01032908  0.02299958]\n",
      "  [ 0.02758487 -0.0527771   0.03682648  0.01091385  0.00823486]\n",
      "  [ 0.03689998 -0.04493044  0.0340664   0.00996787  0.02728124]\n",
      "  [ 0.0201246  -0.04283275  0.03956849  0.01812819  0.01167392]\n",
      "  [ 0.01645836 -0.04893643  0.05292731  0.02730264  0.01676821]\n",
      "  [ 0.02048459 -0.04313716  0.04678174  0.02156526  0.03393343]\n",
      "  [ 0.03501929 -0.04700715  0.04475866  0.0169285   0.03365665]\n",
      "  [ 0.03202622 -0.05200719  0.0412923   0.01421571  0.01357746]\n",
      "  [ 0.02608905 -0.04864164  0.03569085  0.01024603  0.02327376]\n",
      "  [ 0.02763166 -0.05278679  0.03673914  0.01087276  0.00837257]\n",
      "  [ 0.04243734 -0.04659512  0.02809366  0.00697939  0.01543142]\n",
      "  [ 0.02288353 -0.04364774  0.03652432  0.01659998  0.00560123]\n",
      "  [ 0.01784441 -0.04934419  0.05136525  0.02650956  0.01368218]\n",
      "  [ 0.02118291 -0.04333769  0.04599377  0.02115086  0.03237497]\n",
      "  [ 0.03537254 -0.04710543  0.0443574   0.01671537  0.03287284]\n",
      "  [ 0.03220538 -0.0520558   0.04108892  0.01410724  0.0131817 ]\n",
      "  [ 0.02617775 -0.04866577  0.03558894  0.01019084  0.0230715 ]\n",
      "  [ 0.02767666 -0.05279874  0.03668748  0.01084468  0.00827043]\n",
      "  [ 0.04246025 -0.04660107  0.02806728  0.00696516  0.01537934]\n",
      "  [ 0.02289496 -0.04365067  0.03651088  0.01659272  0.00557454]\n",
      "  [ 0.01331869 -0.04222951  0.04086454  0.02140864  0.00063281]\n",
      "  [ 0.02654827 -0.04702105  0.03701976  0.01871293  0.02752496]\n",
      "  [ 0.02548549 -0.04223656  0.03875237  0.01718175  0.03930147]\n",
      "  [ 0.03013033 -0.03730443  0.02262457  0.03068188  0.02840824]\n",
      "  [ 0.02726179 -0.03764378  0.03153032  0.02329288  0.0397392 ]\n",
      "  [ 0.03839153 -0.04442227  0.03701044  0.01779732  0.03654142]\n",
      "  [ 0.03370981 -0.05078015  0.03737447  0.01464778  0.01501678]\n",
      "  [ 0.02690899 -0.04805857  0.03373254  0.01046053  0.02400089]\n",
      "  [ 0.02804096 -0.05251025  0.0357489   0.01097927  0.00873566]\n",
      "  [ 0.03712527 -0.04480297  0.03352247  0.01000081  0.02753338]\n",
      "  [ 0.02023505 -0.042773    0.03929199  0.01814457  0.01180196]\n",
      "  [ 0.01199205 -0.04180074  0.04228204  0.02220187  0.00382391]\n",
      "  [ 0.02588168 -0.04680966  0.03774376  0.01912088  0.02913   ]\n",
      "  [ 0.02514927 -0.04213279  0.03911772  0.0173948   0.04011173]\n",
      "  [ 0.0357133  -0.03606702  0.05090587  0.01981367  0.0378333 ]\n",
      "  [ 0.02996717 -0.03700665  0.04569884  0.01779442  0.04443703]\n",
      "  [ 0.03970604 -0.04409552  0.04417703  0.01505964  0.03887406]\n",
      "  [ 0.03435048 -0.05061162  0.04098258  0.01329915  0.01618018]\n",
      "  [ 0.02721289 -0.04797159  0.03552801  0.00979649  0.02458805]\n",
      "  [ 0.0281887  -0.05246555  0.03665306  0.01065229  0.00902866]\n",
      "  [ 0.03024668 -0.04308404  0.04403923  0.01520283  0.01075727]\n",
      "  [ 0.0168908  -0.04195692  0.04465791  0.02072737  0.00325323]\n",
      "  [ 0.03168176 -0.03600323  0.05374834  0.02142916  0.01917824]\n",
      "  [ 0.02801767 -0.03698694  0.04714731  0.01859162  0.03506033]\n",
      "  [ 0.03876086 -0.04409143  0.04492147  0.01544611  0.03417959]\n",
      "  [ 0.03389093 -0.05061228  0.04136322  0.01348419  0.01382028]\n",
      "  [ 0.02699548 -0.04797332  0.0357204   0.00988486  0.02338731]\n",
      "  [ 0.02808334 -0.05246708  0.03675134  0.01069439  0.00842485]\n",
      "  [ 0.03714576 -0.04478094  0.03402695  0.00985772  0.02737491]\n",
      "  [ 0.0202449  -0.04276195  0.0395477   0.01807358  0.01172053]\n",
      "  [ 0.01199678 -0.04179519  0.04241164  0.02216666  0.00378207]]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 2\n",
    "vocab_size = 19+1\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    for e in range(epoch):\n",
    "        print(e)\n",
    "        sess.run(init)\n",
    "        c = sess.run(model1)\n",
    "        print(\"c:\",c.shape)\n",
    "#         c=tf.repeat(c,[48])\n",
    "#         print(\"c:\",c.shape)\n",
    "        \n",
    "            \n",
    "#         x1=tf.convert_to_tensor(x1)\n",
    "#         se1 = Embedding(vocab_size, 50, mask_zero=True)(x1)\n",
    "#         print(\"se1:\",se1.shape)\n",
    "        inp2 = sess.run(embed)\n",
    "        print(\"inp2:\",inp2.shape)\n",
    "        inp2 = inp2.reshape([1,len(inp2),len(inp2[0])])\n",
    "        print(\"inp2:\",inp2.shape)\n",
    "        test = K.expand_dims(c,1)\n",
    "        print(\"exp dims:\",test.shape)\n",
    "        features_try = K.tile(K.expand_dims(c, 1), [1, K.shape(inp2)[1], 1])\n",
    "        print(\"ktile:\",features_try.shape)\n",
    "#         embeddings = K.concatenate([features_try, inp2], axis=-1)\n",
    "        embeddings = tf.concat([features_try,inp2],2)\n",
    "        \n",
    "        print(\"emb:\",embeddings.shape)\n",
    "        inp = sess.run(embeddings)\n",
    "#         print(x1.shape,x1)\n",
    "#         result = embedding_layer(tf.constant(x1))\n",
    "        \n",
    "#         print(\"res:\",result.shape)\n",
    "        a = sess.run(out3,feed_dict={gru.input_layer:inp})\n",
    "#         b = sess.run(output2,feed_dict={gru2.input_layer:a})\n",
    "    #     loss = \n",
    "        print(\"a:\",a.shape,\"\\n\",a)\n",
    "#         print(\"b:\",b.shape,b)\n",
    "\n",
    "        # ADD SOFTMAX/RELU\n",
    "        # DEFINE LOSS\n",
    "        # MIN LOSS\n",
    "        \n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
